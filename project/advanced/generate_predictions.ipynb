{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb5d7a3",
   "metadata": {},
   "source": [
    "# üéØ Generate Predictions - Advanced Ensemble\n",
    "## Creazione submission per Kaggle utilizzando l'ensemble ottimizzato\n",
    "\n",
    "### Processo:\n",
    "1. Caricare i modelli addestrati\n",
    "2. Processare il prediction_mapping per creare le feature necessarie\n",
    "3. Generare predizioni con ensemble\n",
    "4. Creare file submission nel formato corretto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720be5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"üì¶ Librerie caricate con successo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c9fed1",
   "metadata": {},
   "source": [
    "## 1. Caricamento Modelli e Artefatti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594e0682",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÇ Caricamento modelli...\\n\")\n",
    "\n",
    "# Carica feature columns\n",
    "with open('../models/feature_columns.pkl', 'rb') as f:\n",
    "    feature_cols = pickle.load(f)\n",
    "print(f\"‚úÖ Feature columns: {len(feature_cols)}\")\n",
    "\n",
    "# Carica scaler\n",
    "with open('../models/scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "print(f\"‚úÖ Scaler caricato\")\n",
    "\n",
    "# Carica ensemble weights\n",
    "with open('../models/ensemble_weights.pkl', 'rb') as f:\n",
    "    ensemble_weights = pickle.load(f)\n",
    "print(f\"‚úÖ Ensemble weights caricati:\")\n",
    "for name, weight in ensemble_weights.items():\n",
    "    print(f\"   {name}: {weight:.4f}\")\n",
    "\n",
    "# Carica modelli\n",
    "print(f\"\\nüîß Caricamento modelli...\")\n",
    "\n",
    "# LightGBM\n",
    "lgb_model = lgb.Booster(model_file='../models/lightgbm_quantile_model.txt')\n",
    "print(f\"‚úÖ LightGBM caricato\")\n",
    "\n",
    "# CatBoost\n",
    "catboost_model = CatBoostRegressor()\n",
    "catboost_model.load_model('../models/catboost_quantile_model.cbm')\n",
    "print(f\"‚úÖ CatBoost caricato\")\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "xgb_model.load_model('../models/xgboost_quantile_model.json')\n",
    "print(f\"‚úÖ XGBoost caricato\")\n",
    "\n",
    "# Neural Network\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class QuantileNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[512, 256, 128, 64]):\n",
    "        super(QuantileNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.3))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "nn_model = QuantileNN(input_dim=len(feature_cols)).to(device)\n",
    "nn_model.load_state_dict(torch.load('../models/best_nn_model.pth', map_location=device))\n",
    "nn_model.eval()\n",
    "print(f\"‚úÖ Neural Network caricato (device: {device})\")\n",
    "\n",
    "print(f\"\\nüéâ Tutti i modelli caricati con successo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59c7d4",
   "metadata": {},
   "source": [
    "## 2. Caricamento e Preparazione Dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Caricamento dati storici e prediction mapping...\\n\")\n",
    "\n",
    "# Carica dati processati (per estrarre statistiche storiche)\n",
    "historical_data = pd.read_csv('../data/advanced_features.csv')\n",
    "historical_data['date_arrival'] = pd.to_datetime(historical_data['date_arrival'])\n",
    "\n",
    "print(f\"‚úÖ Dati storici: {historical_data.shape}\")\n",
    "print(f\"   Range: {historical_data['date_arrival'].min()} ‚Üí {historical_data['date_arrival'].max()}\")\n",
    "\n",
    "# Carica prediction mapping\n",
    "prediction_mapping = pd.read_csv('../../data/prediction_mapping.csv')\n",
    "prediction_mapping['forecast_start_date'] = pd.to_datetime(prediction_mapping['forecast_start_date'])\n",
    "prediction_mapping['forecast_end_date'] = pd.to_datetime(prediction_mapping['forecast_end_date'])\n",
    "\n",
    "print(f\"\\n‚úÖ Prediction mapping: {prediction_mapping.shape}\")\n",
    "print(f\"   IDs unici: {prediction_mapping['ID'].nunique()}\")\n",
    "print(f\"   rm_ids unici: {prediction_mapping['rm_id'].nunique()}\")\n",
    "\n",
    "print(\"\\nüìã Prime righe prediction mapping:\")\n",
    "display(prediction_mapping.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a863006",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering per Prediction Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30647cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Creazione feature per prediction set...\\n\")\n",
    "\n",
    "# Per ogni rm_id in prediction_mapping, dobbiamo creare le stesse feature\n",
    "# utilizzate durante il training, basandoci sui dati storici\n",
    "\n",
    "pred_data = prediction_mapping.copy()\n",
    "\n",
    "# ========== STATISTICHE STORICHE PER RM_ID ==========\n",
    "print(\"üìä Estrazione statistiche storiche per rm_id...\")\n",
    "\n",
    "# Per ogni rm_id, estrai le statistiche dall'ultimo periodo storico\n",
    "rm_latest_stats = historical_data.groupby('rm_id').tail(30).groupby('rm_id').agg({\n",
    "    'net_weight': ['mean', 'median', 'std', 'min', 'max'],\n",
    "    'supplier_id': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[-1],\n",
    "    'product_id': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[-1],\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten columns\n",
    "rm_latest_stats.columns = ['rm_id', 'recent_avg_weight', 'recent_median_weight', \n",
    "                            'recent_std_weight', 'recent_min_weight', 'recent_max_weight',\n",
    "                            'most_common_supplier', 'most_common_product']\n",
    "\n",
    "# Merge con prediction data\n",
    "pred_data = pred_data.merge(rm_latest_stats, on='rm_id', how='left')\n",
    "\n",
    "print(f\"‚úÖ Statistiche rm_id aggiunte\")\n",
    "print(f\"   Match rate: {(~pred_data['recent_avg_weight'].isna()).sum() / len(pred_data) * 100:.2f}%\")\n",
    "\n",
    "# ========== TEMPORAL FEATURES ==========\n",
    "print(\"\\n‚è∞ Creazione temporal features...\")\n",
    "\n",
    "# Usa forecast_end_date come data di riferimento\n",
    "pred_data['year'] = pred_data['forecast_end_date'].dt.year\n",
    "pred_data['month'] = pred_data['forecast_end_date'].dt.month\n",
    "pred_data['day'] = pred_data['forecast_end_date'].dt.day\n",
    "pred_data['day_of_week'] = pred_data['forecast_end_date'].dt.dayofweek\n",
    "pred_data['quarter'] = pred_data['forecast_end_date'].dt.quarter\n",
    "pred_data['week_of_year'] = pred_data['forecast_end_date'].dt.isocalendar().week\n",
    "pred_data['is_weekend'] = pred_data['day_of_week'].isin([5, 6]).astype(int)\n",
    "pred_data['is_month_start'] = pred_data['forecast_end_date'].dt.is_month_start.astype(int)\n",
    "pred_data['is_month_end'] = pred_data['forecast_end_date'].dt.is_month_end.astype(int)\n",
    "pred_data['is_quarter_start'] = pred_data['forecast_end_date'].dt.is_quarter_start.astype(int)\n",
    "pred_data['is_quarter_end'] = pred_data['forecast_end_date'].dt.is_quarter_end.astype(int)\n",
    "\n",
    "# Ciclici\n",
    "pred_data['month_sin'] = np.sin(2 * np.pi * pred_data['month'] / 12)\n",
    "pred_data['month_cos'] = np.cos(2 * np.pi * pred_data['month'] / 12)\n",
    "pred_data['day_sin'] = np.sin(2 * np.pi * pred_data['day_of_week'] / 7)\n",
    "pred_data['day_cos'] = np.cos(2 * np.pi * pred_data['day_of_week'] / 7)\n",
    "pred_data['week_sin'] = np.sin(2 * np.pi * pred_data['week_of_year'] / 52)\n",
    "pred_data['week_cos'] = np.cos(2 * np.pi * pred_data['week_of_year'] / 52)\n",
    "\n",
    "# Tempo dall'ultimo dato storico\n",
    "last_historical_date = historical_data['date_arrival'].max()\n",
    "pred_data['days_since_start'] = (pred_data['forecast_end_date'] - historical_data['date_arrival'].min()).dt.days\n",
    "pred_data['weeks_since_start'] = pred_data['days_since_start'] / 7\n",
    "\n",
    "print(\"‚úÖ Temporal features create\")\n",
    "\n",
    "# ========== SUPPLIER & PRODUCT IDs ==========\n",
    "print(\"\\nüë®‚Äçüè≠ Assegnazione supplier_id e product_id...\")\n",
    "\n",
    "pred_data['supplier_id'] = pred_data['most_common_supplier']\n",
    "pred_data['product_id'] = pred_data['most_common_product']\n",
    "\n",
    "# ========== STATISTICHE AGGREGATE STORICHE ==========\n",
    "print(\"\\nüìä Merge con statistiche aggregate storiche...\")\n",
    "\n",
    "# Estrai le statistiche aggregate finali dal dataset storico\n",
    "# (queste sono gi√† presenti nel dataset processato)\n",
    "stat_cols = [col for col in historical_data.columns if any(x in col for x in \n",
    "             ['supplier_mean', 'supplier_median', 'supplier_std', 'supplier_cv', 'supplier_count',\n",
    "              'rm_mean', 'rm_std', 'rm_count', 'rm_cv',\n",
    "              'product_mean', 'product_std', 'product_count', 'product_cv'])]\n",
    "\n",
    "# Per ogni rm_id, prendi le ultime statistiche disponibili\n",
    "latest_stats = historical_data.groupby('rm_id').last()[stat_cols].reset_index()\n",
    "pred_data = pred_data.merge(latest_stats, on='rm_id', how='left')\n",
    "\n",
    "print(\"‚úÖ Statistiche aggregate aggiunte\")\n",
    "\n",
    "# ========== LAG FEATURES (dalle ultime ricezioni) ==========\n",
    "print(\"\\n‚è≥ Creazione lag features...\")\n",
    "\n",
    "# Per ogni rm_id, estrai le ultime N ricezioni\n",
    "lag_features = {}\n",
    "for lag in [1, 2, 3, 5, 7, 14]:\n",
    "    lag_col = f'weight_lag_{lag}'\n",
    "    lag_dict = historical_data.groupby('rm_id')['net_weight'].apply(\n",
    "        lambda x: x.iloc[-lag] if len(x) >= lag else x.mean()\n",
    "    ).to_dict()\n",
    "    pred_data[lag_col] = pred_data['rm_id'].map(lag_dict)\n",
    "\n",
    "# Rolling statistics\n",
    "for window in [3, 7, 14, 30]:\n",
    "    rolling_mean_col = f'rolling_mean_{window}'\n",
    "    rolling_mean_dict = historical_data.groupby('rm_id')['net_weight'].apply(\n",
    "        lambda x: x.tail(window).mean()\n",
    "    ).to_dict()\n",
    "    pred_data[rolling_mean_col] = pred_data['rm_id'].map(rolling_mean_dict)\n",
    "    \n",
    "    rolling_std_col = f'rolling_std_{window}'\n",
    "    rolling_std_dict = historical_data.groupby('rm_id')['net_weight'].apply(\n",
    "        lambda x: x.tail(window).std()\n",
    "    ).to_dict()\n",
    "    pred_data[rolling_std_col] = pred_data['rm_id'].map(rolling_std_dict)\n",
    "    \n",
    "    rolling_min_col = f'rolling_min_{window}'\n",
    "    rolling_min_dict = historical_data.groupby('rm_id')['net_weight'].apply(\n",
    "        lambda x: x.tail(window).min()\n",
    "    ).to_dict()\n",
    "    pred_data[rolling_min_col] = pred_data['rm_id'].map(rolling_min_dict)\n",
    "    \n",
    "    rolling_max_col = f'rolling_max_{window}'\n",
    "    rolling_max_dict = historical_data.groupby('rm_id')['net_weight'].apply(\n",
    "        lambda x: x.tail(window).max()\n",
    "    ).to_dict()\n",
    "    pred_data[rolling_max_col] = pred_data['rm_id'].map(rolling_max_dict)\n",
    "\n",
    "print(\"‚úÖ Lag e rolling features create\")\n",
    "\n",
    "# ========== INTERACTION FEATURES ==========\n",
    "print(\"\\nüîÄ Creazione interaction features...\")\n",
    "\n",
    "# Combinazioni\n",
    "pred_data['supplier_rm_freq'] = 0  # Default\n",
    "pred_data['supplier_product_freq'] = 0\n",
    "pred_data['rm_product_freq'] = 0\n",
    "pred_data['supplier_rm_mean'] = pred_data['recent_avg_weight']\n",
    "pred_data['supplier_product_mean'] = pred_data['recent_avg_weight']\n",
    "pred_data['rm_product_mean'] = pred_data['recent_avg_weight']\n",
    "\n",
    "# Deviazioni e ratio\n",
    "pred_data['deviation_from_supplier_mean'] = 0\n",
    "pred_data['deviation_from_rm_mean'] = 0\n",
    "pred_data['deviation_from_product_mean'] = 0\n",
    "pred_data['weight_to_supplier_mean_ratio'] = 1.0\n",
    "pred_data['weight_to_rm_mean_ratio'] = 1.0\n",
    "pred_data['weight_to_product_mean_ratio'] = 1.0\n",
    "\n",
    "# Trend\n",
    "pred_data['trend_3_vs_14'] = pred_data['rolling_mean_3'] / (pred_data['rolling_mean_14'] + 1e-6)\n",
    "pred_data['trend_7_vs_30'] = pred_data['rolling_mean_7'] / (pred_data['rolling_mean_30'] + 1e-6)\n",
    "pred_data['momentum_1_3'] = pred_data['weight_lag_1'] - pred_data['weight_lag_3']\n",
    "pred_data['momentum_3_7'] = pred_data['weight_lag_3'] - pred_data['weight_lag_7']\n",
    "\n",
    "# Recency\n",
    "pred_data['days_since_last_receival'] = (pred_data['forecast_end_date'] - last_historical_date).dt.days\n",
    "pred_data['supplier_avg_days_between'] = 7.0  # Default\n",
    "\n",
    "print(\"‚úÖ Interaction features create\")\n",
    "\n",
    "# ========== ALTRE FEATURE MANCANTI ==========\n",
    "# Aggiungi le feature che potrebbero mancare rispetto al training set\n",
    "missing_features = [col for col in feature_cols if col not in pred_data.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\n‚ö†Ô∏è Aggiungo {len(missing_features)} feature mancanti con valori di default...\")\n",
    "    for col in missing_features:\n",
    "        # Valori di default sensati\n",
    "        if 'ratio' in col or 'trend' in col:\n",
    "            pred_data[col] = 1.0\n",
    "        elif 'cv' in col or 'std' in col:\n",
    "            pred_data[col] = 0.0\n",
    "        elif 'encoded' in col:\n",
    "            pred_data[col] = 0\n",
    "        else:\n",
    "            pred_data[col] = 0.0\n",
    "\n",
    "print(f\"\\n‚úÖ Feature engineering completato!\")\n",
    "print(f\"   Shape: {pred_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d5ddfd",
   "metadata": {},
   "source": [
    "## 4. Gestione Valori Mancanti e Preparazione Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0383409",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Gestione valori mancanti...\\n\")\n",
    "\n",
    "# Riempi NaN con valori sensati (come fatto nel training)\n",
    "lag_cols = [col for col in pred_data.columns if 'lag_' in col or 'rolling_' in col or 'momentum_' in col]\n",
    "for col in lag_cols:\n",
    "    if col in pred_data.columns:\n",
    "        pred_data[col] = pred_data[col].fillna(pred_data[col].median())\n",
    "\n",
    "ratio_cols = [col for col in pred_data.columns if 'ratio' in col or 'trend_' in col]\n",
    "for col in ratio_cols:\n",
    "    if col in pred_data.columns:\n",
    "        pred_data[col] = pred_data[col].fillna(1.0)\n",
    "\n",
    "cv_cols = [col for col in pred_data.columns if '_cv' in col]\n",
    "for col in cv_cols:\n",
    "    if col in pred_data.columns:\n",
    "        pred_data[col] = pred_data[col].fillna(0)\n",
    "\n",
    "# Riempi tutti i rimanenti NaN con 0\n",
    "pred_data = pred_data.fillna(0)\n",
    "\n",
    "print(f\"‚úÖ Valori mancanti gestiti\")\n",
    "print(f\"   NaN rimanenti: {pred_data[feature_cols].isnull().sum().sum()}\")\n",
    "\n",
    "# Prepara feature matrix\n",
    "X_pred = pred_data[feature_cols].values\n",
    "\n",
    "print(f\"\\nüìä Feature matrix per predizione:\")\n",
    "print(f\"   Shape: {X_pred.shape}\")\n",
    "print(f\"   Samples: {X_pred.shape[0]}\")\n",
    "print(f\"   Features: {X_pred.shape[1]}\")\n",
    "\n",
    "# Verifica finale\n",
    "if np.isnan(X_pred).any():\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: Ci sono ancora NaN nella feature matrix!\")\n",
    "    nan_cols = [feature_cols[i] for i in range(len(feature_cols)) if np.isnan(X_pred[:, i]).any()]\n",
    "    print(f\"   Colonne con NaN: {nan_cols[:10]}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Nessun NaN nella feature matrix!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641068fb",
   "metadata": {},
   "source": [
    "## 5. Generazione Predizioni con Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c6bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÆ Generazione predizioni con ensemble...\\n\")\n",
    "\n",
    "# ========== LightGBM ==========\n",
    "print(\"üåü Predicting con LightGBM...\")\n",
    "lgb_predictions = lgb_model.predict(X_pred)\n",
    "print(f\"   Min: {lgb_predictions.min():.2f} | Max: {lgb_predictions.max():.2f} | Mean: {lgb_predictions.mean():.2f}\")\n",
    "\n",
    "# ========== CatBoost ==========\n",
    "print(\"\\nüêà Predicting con CatBoost...\")\n",
    "catboost_predictions = catboost_model.predict(X_pred)\n",
    "print(f\"   Min: {catboost_predictions.min():.2f} | Max: {catboost_predictions.max():.2f} | Mean: {catboost_predictions.mean():.2f}\")\n",
    "\n",
    "# ========== XGBoost ==========\n",
    "print(\"\\nüöÄ Predicting con XGBoost...\")\n",
    "xgb_predictions = xgb_model.predict(X_pred)\n",
    "print(f\"   Min: {xgb_predictions.min():.2f} | Max: {xgb_predictions.max():.2f} | Mean: {xgb_predictions.mean():.2f}\")\n",
    "\n",
    "# ========== Neural Network ==========\n",
    "print(\"\\nüß† Predicting con Neural Network...\")\n",
    "X_pred_scaled = scaler.transform(X_pred)\n",
    "X_pred_tensor = torch.FloatTensor(X_pred_scaled).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    nn_predictions = nn_model(X_pred_tensor).cpu().numpy()\n",
    "\n",
    "print(f\"   Min: {nn_predictions.min():.2f} | Max: {nn_predictions.max():.2f} | Mean: {nn_predictions.mean():.2f}\")\n",
    "\n",
    "# ========== ENSEMBLE ==========\n",
    "print(\"\\nüé≠ Creazione ensemble...\")\n",
    "\n",
    "ensemble_predictions = (\n",
    "    ensemble_weights['LightGBM'] * lgb_predictions +\n",
    "    ensemble_weights['CatBoost'] * catboost_predictions +\n",
    "    ensemble_weights['XGBoost'] * xgb_predictions +\n",
    "    ensemble_weights['NeuralNet'] * nn_predictions\n",
    ")\n",
    "\n",
    "# Clip valori negativi a 0 (non pu√≤ esistere peso negativo)\n",
    "negative_count = (ensemble_predictions < 0).sum()\n",
    "if negative_count > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Clipping {negative_count} predizioni negative a 0\")\n",
    "    ensemble_predictions = np.maximum(ensemble_predictions, 0)\n",
    "\n",
    "print(f\"\\n‚úÖ Ensemble predictions:\")\n",
    "print(f\"   Min: {ensemble_predictions.min():.2f}\")\n",
    "print(f\"   Max: {ensemble_predictions.max():.2f}\")\n",
    "print(f\"   Mean: {ensemble_predictions.mean():.2f}\")\n",
    "print(f\"   Median: {np.median(ensemble_predictions):.2f}\")\n",
    "print(f\"   Std: {ensemble_predictions.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c2428d",
   "metadata": {},
   "source": [
    "## 6. Creazione Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716124b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìù Creazione submission file...\\n\")\n",
    "\n",
    "# Crea dataframe submission\n",
    "submission = pd.DataFrame({\n",
    "    'ID': pred_data['ID'],\n",
    "    'predicted_weight': ensemble_predictions\n",
    "})\n",
    "\n",
    "# Verifica formato\n",
    "print(f\"‚úÖ Submission creata:\")\n",
    "print(f\"   Shape: {submission.shape}\")\n",
    "print(f\"   Colonne: {submission.columns.tolist()}\")\n",
    "print(f\"   IDs unici: {submission['ID'].nunique()}\")\n",
    "print(f\"\\nüìã Prime righe:\")\n",
    "display(submission.head(10))\n",
    "\n",
    "# Verifica che tutti gli ID siano presenti\n",
    "required_ids = set(prediction_mapping['ID'])\n",
    "submission_ids = set(submission['ID'])\n",
    "\n",
    "if required_ids == submission_ids:\n",
    "    print(f\"\\n‚úÖ Tutti gli ID richiesti sono presenti!\")\n",
    "else:\n",
    "    missing = required_ids - submission_ids\n",
    "    extra = submission_ids - required_ids\n",
    "    if missing:\n",
    "        print(f\"\\n‚ö†Ô∏è ID mancanti: {len(missing)}\")\n",
    "    if extra:\n",
    "        print(f\"\\n‚ö†Ô∏è ID extra: {len(extra)}\")\n",
    "\n",
    "# Verifica valori\n",
    "if submission['predicted_weight'].isnull().any():\n",
    "    print(f\"\\n‚ùå WARNING: Ci sono valori NaN nelle predizioni!\")\n",
    "    print(f\"   Count: {submission['predicted_weight'].isnull().sum()}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Nessun valore NaN nelle predizioni\")\n",
    "\n",
    "if (submission['predicted_weight'] < 0).any():\n",
    "    print(f\"\\n‚ùå WARNING: Ci sono valori negativi!\")\n",
    "    print(f\"   Count: {(submission['predicted_weight'] < 0).sum()}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Nessun valore negativo\")\n",
    "\n",
    "# Salva submission\n",
    "output_path = '../data/submission_advanced_ensemble.csv'\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nüíæ Submission salvata in: {output_path}\")\n",
    "print(f\"\\nüéâ Processo completato con successo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980a0d42",
   "metadata": {},
   "source": [
    "## 7. Statistiche e Visualizzazioni Finali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace13da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üìä Analisi finale delle predizioni...\\n\")\n",
    "\n",
    "# Statistiche dettagliate\n",
    "print(f\"üìà Statistiche predizioni:\")\n",
    "print(submission['predicted_weight'].describe())\n",
    "\n",
    "# Confronto con dati storici\n",
    "historical_weights = historical_data['net_weight']\n",
    "print(f\"\\nüìä Confronto con dati storici:\")\n",
    "print(f\"   Historical mean: {historical_weights.mean():.2f}\")\n",
    "print(f\"   Predicted mean: {submission['predicted_weight'].mean():.2f}\")\n",
    "print(f\"   Difference: {(submission['predicted_weight'].mean() - historical_weights.mean()):.2f}\")\n",
    "\n",
    "# Visualizzazioni\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Distribuzione predizioni\n",
    "axes[0, 0].hist(submission['predicted_weight'], bins=100, edgecolor='black', alpha=0.7, label='Predicted')\n",
    "axes[0, 0].hist(historical_weights, bins=100, edgecolor='black', alpha=0.5, label='Historical')\n",
    "axes[0, 0].set_xlabel('Weight (kg)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution: Predicted vs Historical')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Boxplot\n",
    "data_to_plot = [historical_weights, submission['predicted_weight']]\n",
    "axes[0, 1].boxplot(data_to_plot, labels=['Historical', 'Predicted'])\n",
    "axes[0, 1].set_ylabel('Weight (kg)')\n",
    "axes[0, 1].set_title('Boxplot Comparison')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Predizioni per rm_id (top 10 rm_ids)\n",
    "top_rm_ids = pred_data['rm_id'].value_counts().head(10).index\n",
    "for rm_id in top_rm_ids[:5]:  # Solo primi 5 per leggibilit√†\n",
    "    rm_preds = submission[pred_data['rm_id'] == rm_id]['predicted_weight']\n",
    "    axes[1, 0].plot(rm_preds.values[:50], alpha=0.7, label=f'rm_id {rm_id}')\n",
    "\n",
    "axes[1, 0].set_xlabel('Sample Index')\n",
    "axes[1, 0].set_ylabel('Predicted Weight (kg)')\n",
    "axes[1, 0].set_title('Predictions by rm_id (Top 5, first 50 samples)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. Percentili\n",
    "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "hist_percentiles = [np.percentile(historical_weights, p) for p in percentiles]\n",
    "pred_percentiles = [np.percentile(submission['predicted_weight'], p) for p in percentiles]\n",
    "\n",
    "x = np.arange(len(percentiles))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 1].bar(x - width/2, hist_percentiles, width, label='Historical', alpha=0.8)\n",
    "axes[1, 1].bar(x + width/2, pred_percentiles, width, label='Predicted', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Percentile')\n",
    "axes[1, 1].set_ylabel('Weight (kg)')\n",
    "axes[1, 1].set_title('Percentile Comparison')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels([f'{p}%' for p in percentiles])\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/submission_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüíæ Grafici salvati in: ../data/submission_analysis.png\")\n",
    "print(f\"\\nüèÅ Analisi completata!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
