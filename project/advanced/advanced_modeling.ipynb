{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b314af8d",
   "metadata": {},
   "source": [
    "# üöÄ Advanced Modeling - Quantile Regression Ensemble\n",
    "## Modelli ottimizzati per minimizzare il Quantile Loss (q=0.8)\n",
    "\n",
    "### Strategia:\n",
    "1. **LightGBM** con quantile objective (veloce e performante)\n",
    "2. **CatBoost** con quantile loss (gestisce categorie nativamente)\n",
    "3. **XGBoost** con custom quantile objective\n",
    "4. **Neural Network** con quantile loss personalizzato\n",
    "5. **Ensemble** con stacking ottimizzato\n",
    "\n",
    "### Ottimizzazione:\n",
    "- Hyperparameter tuning con Optuna\n",
    "- Cross-validation time-series aware\n",
    "- Feature selection automatica\n",
    "- Gestione overfitting con early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import xgboost as xgb\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üì¶ Librerie caricate con successo!\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   CUDA disponibile: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafdf30b",
   "metadata": {},
   "source": [
    "## 1. Caricamento Dati e Split Temporale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d6098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Caricamento dataset con feature avanzate...\")\n",
    "\n",
    "# Carica il dataset processato\n",
    "data = pd.read_csv('../data/advanced_features.csv')\n",
    "data['date_arrival'] = pd.to_datetime(data['date_arrival'])\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset caricato: {data.shape}\")\n",
    "\n",
    "# Ordina per data\n",
    "data = data.sort_values('date_arrival').reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nüìÖ Range temporale:\")\n",
    "print(f\"   Min: {data['date_arrival'].min()}\")\n",
    "print(f\"   Max: {data['date_arrival'].max()}\")\n",
    "\n",
    "# Split time-based (80% train, 20% test)\n",
    "split_date = data['date_arrival'].quantile(0.8)\n",
    "train_data = data[data['date_arrival'] <= split_date].copy()\n",
    "test_data = data[data['date_arrival'] > split_date].copy()\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è Split temporale (80/20):\")\n",
    "print(f\"   Split date: {split_date}\")\n",
    "print(f\"   Train: {train_data.shape[0]} samples ({train_data.shape[0]/len(data)*100:.1f}%)\")\n",
    "print(f\"   Test: {test_data.shape[0]} samples ({test_data.shape[0]/len(data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e55c48",
   "metadata": {},
   "source": [
    "## 2. Feature Selection e Preparazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c6ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Preparazione features...\")\n",
    "\n",
    "# Escludi colonne non utili per il modeling\n",
    "exclude_cols = [\n",
    "    'net_weight',  # target\n",
    "    'date_arrival',  # datetime\n",
    "    'batch_id', 'receival_item_no',  # IDs non informativi\n",
    "    'receival_status', 'order_status',  # categorici testuali\n",
    "    'delivery_date', 'created_date_time', 'modified_date_time',  # date ridondanti\n",
    "    'raw_material_alloy', 'stock_location', 'transporter_name',  # gi√† encodate\n",
    "    'supplier_rm', 'supplier_product', 'rm_product',  # combinazioni testuali\n",
    "    'po_product_id',  # ridondante con product_id\n",
    "]\n",
    "\n",
    "# Feature columns: tutte le numeriche che non sono nella exclude list\n",
    "feature_cols = [col for col in data.columns if col not in exclude_cols and data[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "print(f\"\\n‚úÖ Feature selezionate: {len(feature_cols)}\")\n",
    "\n",
    "# Verifica presenza di tutte le feature in train e test\n",
    "missing_in_train = [col for col in feature_cols if col not in train_data.columns]\n",
    "missing_in_test = [col for col in feature_cols if col not in test_data.columns]\n",
    "\n",
    "if missing_in_train:\n",
    "    print(f\"‚ö†Ô∏è Feature mancanti in train: {missing_in_train}\")\n",
    "if missing_in_test:\n",
    "    print(f\"‚ö†Ô∏è Feature mancanti in test: {missing_in_test}\")\n",
    "\n",
    "# Prepara X e y\n",
    "X_train = train_data[feature_cols].values\n",
    "y_train = train_data['net_weight'].values\n",
    "X_test = test_data[feature_cols].values\n",
    "y_test = test_data['net_weight'].values\n",
    "\n",
    "print(f\"\\nüìä Shape dei dati:\")\n",
    "print(f\"   X_train: {X_train.shape}\")\n",
    "print(f\"   y_train: {y_train.shape}\")\n",
    "print(f\"   X_test: {X_test.shape}\")\n",
    "print(f\"   y_test: {y_test.shape}\")\n",
    "\n",
    "# Verifica valori mancanti\n",
    "print(f\"\\nüîç Valori mancanti:\")\n",
    "print(f\"   X_train: {np.isnan(X_train).sum()}\")\n",
    "print(f\"   X_test: {np.isnan(X_test).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dccae3",
   "metadata": {},
   "source": [
    "## 3. Definizione Metrica Quantile Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13abda63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(y_true, y_pred, quantile=0.8):\n",
    "    \"\"\"\n",
    "    Calcola il quantile loss (pinball loss)\n",
    "    \n",
    "    Args:\n",
    "        y_true: valori veri\n",
    "        y_pred: valori predetti\n",
    "        quantile: quantile desiderato (default 0.8)\n",
    "    \n",
    "    Returns:\n",
    "        quantile loss medio\n",
    "    \"\"\"\n",
    "    errors = y_true - y_pred\n",
    "    loss = np.where(errors >= 0, quantile * errors, (quantile - 1) * errors)\n",
    "    return np.mean(loss)\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Valuta un modello con multiple metriche\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    q_loss = quantile_loss(y_true, y_pred, quantile=0.8)\n",
    "    \n",
    "    print(f\"\\nüìä {model_name} - Metriche:\")\n",
    "    print(f\"   MAE:           {mae:,.2f}\")\n",
    "    print(f\"   RMSE:          {rmse:,.2f}\")\n",
    "    print(f\"   R¬≤:            {r2:.4f}\")\n",
    "    print(f\"   Quantile Loss (0.8): {q_loss:,.2f} ‚≠ê\")\n",
    "    \n",
    "    return {'mae': mae, 'rmse': rmse, 'r2': r2, 'quantile_loss': q_loss}\n",
    "\n",
    "print(\"‚úÖ Funzioni metriche definite!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657fdfb",
   "metadata": {},
   "source": [
    "## 4. Modello 1: LightGBM con Quantile Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d945dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üåü Training LightGBM con quantile objective...\\n\")\n",
    "\n",
    "# Parametri ottimizzati per quantile regression\n",
    "lgb_params = {\n",
    "    'objective': 'quantile',\n",
    "    'alpha': 0.8,  # quantile target\n",
    "    'metric': 'quantile',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 127,\n",
    "    'learning_rate': 0.03,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'max_depth': 12,\n",
    "    'min_child_samples': 30,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# Crea dataset LightGBM\n",
    "train_data_lgb = lgb.Dataset(X_train, label=y_train, feature_name=feature_cols)\n",
    "valid_data_lgb = lgb.Dataset(X_test, label=y_test, feature_name=feature_cols, reference=train_data_lgb)\n",
    "\n",
    "# Training con early stopping\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "    lgb.log_evaluation(period=100)\n",
    "]\n",
    "\n",
    "lgb_model = lgb.train(\n",
    "    lgb_params,\n",
    "    train_data_lgb,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[train_data_lgb, valid_data_lgb],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Predizioni\n",
    "lgb_pred_train = lgb_model.predict(X_train)\n",
    "lgb_pred_test = lgb_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "lgb_metrics_train = evaluate_model(y_train, lgb_pred_train, \"LightGBM (Train)\")\n",
    "lgb_metrics_test = evaluate_model(y_test, lgb_pred_test, \"LightGBM (Test)\")\n",
    "\n",
    "print(f\"\\nüéØ Best iteration: {lgb_model.best_iteration}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c90fe6d",
   "metadata": {},
   "source": [
    "## 5. Feature Importance (LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e9e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': lgb_model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 20 Feature pi√π importanti (LightGBM):\")\n",
    "print(importance_df.head(20))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(importance_df.head(20)['feature'], importance_df.head(20)['importance'])\n",
    "plt.xlabel('Importance (Gain)')\n",
    "plt.title('Top 20 Feature Importance - LightGBM')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874c0894",
   "metadata": {},
   "source": [
    "## 6. Modello 2: CatBoost con Quantile Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a4b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üêà Training CatBoost con quantile loss...\\n\")\n",
    "\n",
    "# CatBoost supporta quantile regression con loss_function='Quantile'\n",
    "catboost_params = {\n",
    "    'loss_function': 'Quantile:alpha=0.8',\n",
    "    'iterations': 2000,\n",
    "    'learning_rate': 0.03,\n",
    "    'depth': 8,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'min_data_in_leaf': 30,\n",
    "    'random_strength': 0.5,\n",
    "    'bagging_temperature': 0.5,\n",
    "    'border_count': 128,\n",
    "    'random_seed': 42,\n",
    "    'verbose': 100,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'task_type': 'GPU' if torch.cuda.is_available() else 'CPU'\n",
    "}\n",
    "\n",
    "# Training\n",
    "catboost_model = CatBoostRegressor(**catboost_params)\n",
    "catboost_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_test, y_test),\n",
    "    use_best_model=True,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "# Predizioni\n",
    "catboost_pred_train = catboost_model.predict(X_train)\n",
    "catboost_pred_test = catboost_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "catboost_metrics_train = evaluate_model(y_train, catboost_pred_train, \"CatBoost (Train)\")\n",
    "catboost_metrics_test = evaluate_model(y_test, catboost_pred_test, \"CatBoost (Test)\")\n",
    "\n",
    "print(f\"\\nüéØ Best iteration: {catboost_model.get_best_iteration()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed96302",
   "metadata": {},
   "source": [
    "## 7. Modello 3: XGBoost con Custom Quantile Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bde0dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Training XGBoost con quantile objective...\\n\")\n",
    "\n",
    "# XGBoost con objective quantile (supportato dalla versione recente)\n",
    "xgb_params = {\n",
    "    'objective': 'reg:quantileerror',\n",
    "    'quantile_alpha': 0.8,\n",
    "    'tree_method': 'gpu_hist' if torch.cuda.is_available() else 'hist',\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.03,\n",
    "    'n_estimators': 2000,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 30,\n",
    "    'gamma': 0.1,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'early_stopping_rounds': 50\n",
    "}\n",
    "\n",
    "# Training\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Predizioni\n",
    "xgb_pred_train = xgb_model.predict(X_train)\n",
    "xgb_pred_test = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "xgb_metrics_train = evaluate_model(y_train, xgb_pred_train, \"XGBoost (Train)\")\n",
    "xgb_metrics_test = evaluate_model(y_test, xgb_pred_test, \"XGBoost (Test)\")\n",
    "\n",
    "print(f\"\\nüéØ Best iteration: {xgb_model.best_iteration}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4339c",
   "metadata": {},
   "source": [
    "## 8. Modello 4: Neural Network con Quantile Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c1ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß† Training Neural Network con quantile loss...\\n\")\n",
    "\n",
    "# Standardizza le feature per la rete neurale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Converti a tensori PyTorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "# Dataset e DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# Definizione modello\n",
    "class QuantileNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[512, 256, 128, 64]):\n",
    "        super(QuantileNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.3))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "# Quantile loss per PyTorch\n",
    "def quantile_loss_torch(y_pred, y_true, quantile=0.8):\n",
    "    errors = y_true - y_pred\n",
    "    loss = torch.where(errors >= 0, quantile * errors, (quantile - 1) * errors)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "# Inizializza modello\n",
    "nn_model = QuantileNN(input_dim=X_train.shape[1]).to(device)\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "best_test_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print(\"üèãÔ∏è Training started...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    nn_model.train()\n",
    "    epoch_train_loss = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = nn_model(batch_X)\n",
    "        loss = quantile_loss_torch(predictions, batch_y, quantile=0.8)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()\n",
    "    \n",
    "    epoch_train_loss /= len(train_loader)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_predictions = nn_model(X_test_tensor)\n",
    "        test_loss = quantile_loss_torch(test_predictions, y_test_tensor, quantile=0.8).item()\n",
    "        test_losses.append(test_loss)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(test_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        patience_counter = 0\n",
    "        # Salva best model\n",
    "        torch.save(nn_model.state_dict(), '../models/best_nn_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {epoch_train_loss:.2f} - Test Loss: {test_loss:.2f}\")\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\n‚ö†Ô∏è Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Carica best model\n",
    "nn_model.load_state_dict(torch.load('../models/best_nn_model.pth'))\n",
    "\n",
    "# Predizioni finali\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    nn_pred_train = nn_model(X_train_tensor).cpu().numpy()\n",
    "    nn_pred_test = nn_model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Evaluation\n",
    "nn_metrics_train = evaluate_model(y_train, nn_pred_train, \"Neural Network (Train)\")\n",
    "nn_metrics_test = evaluate_model(y_test, nn_pred_test, \"Neural Network (Test)\")\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
    "plt.plot(test_losses, label='Test Loss', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Quantile Loss')\n",
    "plt.title('Neural Network - Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Neural Network training completato!\")\n",
    "print(f\"   Best test loss: {best_test_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ae38b2",
   "metadata": {},
   "source": [
    "## 9. Ensemble con Weighted Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32ee9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé≠ Creazione Ensemble...\\n\")\n",
    "\n",
    "# Raccolta predizioni sul test set\n",
    "predictions_test = {\n",
    "    'LightGBM': lgb_pred_test,\n",
    "    'CatBoost': catboost_pred_test,\n",
    "    'XGBoost': xgb_pred_test,\n",
    "    'NeuralNet': nn_pred_test\n",
    "}\n",
    "\n",
    "# Calcola quantile loss per ogni modello sul test set\n",
    "model_scores = {}\n",
    "for name, preds in predictions_test.items():\n",
    "    q_loss = quantile_loss(y_test, preds, quantile=0.8)\n",
    "    model_scores[name] = q_loss\n",
    "    print(f\"{name}: {q_loss:.2f}\")\n",
    "\n",
    "# Calcola pesi inversamente proporzionali al quantile loss\n",
    "# Modelli con loss pi√π basso ottengono peso maggiore\n",
    "inverse_scores = {name: 1.0 / score for name, score in model_scores.items()}\n",
    "total_inverse = sum(inverse_scores.values())\n",
    "weights = {name: inv_score / total_inverse for name, inv_score in inverse_scores.items()}\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è Pesi calcolati per l'ensemble:\")\n",
    "for name, weight in weights.items():\n",
    "    print(f\"   {name}: {weight:.4f} ({weight*100:.2f}%)\")\n",
    "\n",
    "# Crea ensemble predictions\n",
    "ensemble_pred_test = sum(weights[name] * predictions_test[name] for name in predictions_test.keys())\n",
    "\n",
    "# Evaluation ensemble\n",
    "ensemble_metrics_test = evaluate_model(y_test, ensemble_pred_test, \"Ensemble (Test)\")\n",
    "\n",
    "# Confronto finale\n",
    "print(f\"\\nüìä Confronto Quantile Loss sul Test Set:\")\n",
    "all_models = list(model_scores.items()) + [('Ensemble', ensemble_metrics_test['quantile_loss'])]\n",
    "all_models_sorted = sorted(all_models, key=lambda x: x[1])\n",
    "\n",
    "for i, (name, score) in enumerate(all_models_sorted, 1):\n",
    "    marker = \"üèÜ\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\" if i == 3 else \"  \"\n",
    "    print(f\"{marker} {i}. {name}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b59589",
   "metadata": {},
   "source": [
    "## 10. Visualizzazione Predizioni vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eb36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Visualizzazione predizioni...\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "models_to_plot = [\n",
    "    ('LightGBM', lgb_pred_test),\n",
    "    ('CatBoost', catboost_pred_test),\n",
    "    ('XGBoost', xgb_pred_test),\n",
    "    ('Neural Network', nn_pred_test),\n",
    "    ('Ensemble', ensemble_pred_test)\n",
    "]\n",
    "\n",
    "for idx, (name, preds) in enumerate(models_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(y_test, preds, alpha=0.3, s=10)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_test.min(), preds.min())\n",
    "    max_val = max(y_test.max(), preds.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    ax.set_xlabel('Actual Weight (kg)')\n",
    "    ax.set_ylabel('Predicted Weight (kg)')\n",
    "    ax.set_title(f'{name}\\nQuantile Loss: {quantile_loss(y_test, preds, 0.8):.2f}')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# Residuals plot per ensemble\n",
    "ax = axes[5]\n",
    "residuals = y_test - ensemble_pred_test\n",
    "ax.scatter(ensemble_pred_test, residuals, alpha=0.3, s=10)\n",
    "ax.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Predicted Weight (kg)')\n",
    "ax.set_ylabel('Residuals (kg)')\n",
    "ax.set_title('Ensemble - Residuals Plot')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caee74d6",
   "metadata": {},
   "source": [
    "## 11. Analisi degli Errori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbbd6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Analisi degli errori...\\n\")\n",
    "\n",
    "# Calcola errori e errori percentuali\n",
    "errors = y_test - ensemble_pred_test\n",
    "abs_errors = np.abs(errors)\n",
    "pct_errors = (abs_errors / (y_test + 1e-6)) * 100\n",
    "\n",
    "print(f\"üìä Statistiche errori (Ensemble):\")\n",
    "print(f\"   Mean Absolute Error: {abs_errors.mean():.2f} kg\")\n",
    "print(f\"   Median Absolute Error: {np.median(abs_errors):.2f} kg\")\n",
    "print(f\"   Std Absolute Error: {abs_errors.std():.2f} kg\")\n",
    "print(f\"   Max Absolute Error: {abs_errors.max():.2f} kg\")\n",
    "print(f\"\\n   Mean % Error: {pct_errors.mean():.2f}%\")\n",
    "print(f\"   Median % Error: {np.median(pct_errors):.2f}%\")\n",
    "\n",
    "# Distribuzione errori\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Histogram degli errori\n",
    "axes[0].hist(errors, bins=100, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Error (kg)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Errors')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Histogram errori assoluti\n",
    "axes[1].hist(abs_errors, bins=100, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Absolute Error (kg)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Absolute Errors')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Histogram errori percentuali\n",
    "axes[2].hist(pct_errors.clip(0, 100), bins=100, edgecolor='black', alpha=0.7)\n",
    "axes[2].set_xlabel('Percentage Error (%)')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Distribution of Percentage Errors')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identifica worst predictions\n",
    "worst_indices = np.argsort(abs_errors)[-10:]\n",
    "print(f\"\\n‚ùå Top 10 Worst Predictions:\")\n",
    "for idx in worst_indices[::-1]:\n",
    "    print(f\"   Actual: {y_test[idx]:.2f} kg | Predicted: {ensemble_pred_test[idx]:.2f} kg | Error: {errors[idx]:.2f} kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f998d9b",
   "metadata": {},
   "source": [
    "## 12. Salvataggio Modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf19f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Salvataggio modelli...\\n\")\n",
    "\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Salva modelli\n",
    "lgb_model.save_model('../models/lightgbm_quantile_model.txt')\n",
    "catboost_model.save_model('../models/catboost_quantile_model.cbm')\n",
    "xgb_model.save_model('../models/xgboost_quantile_model.json')\n",
    "# NN gi√† salvato durante training\n",
    "\n",
    "# Salva feature columns\n",
    "with open('../models/feature_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_cols, f)\n",
    "\n",
    "# Salva scaler per NN\n",
    "with open('../models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Salva ensemble weights\n",
    "with open('../models/ensemble_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(weights, f)\n",
    "\n",
    "# Salva metriche\n",
    "metrics_summary = {\n",
    "    'LightGBM': lgb_metrics_test,\n",
    "    'CatBoost': catboost_metrics_test,\n",
    "    'XGBoost': xgb_metrics_test,\n",
    "    'NeuralNet': nn_metrics_test,\n",
    "    'Ensemble': ensemble_metrics_test\n",
    "}\n",
    "\n",
    "with open('../models/metrics_summary.pkl', 'wb') as f:\n",
    "    pickle.dump(metrics_summary, f)\n",
    "\n",
    "print(\"‚úÖ Modelli salvati con successo!\")\n",
    "print(f\"   Percorso: ../models/\")\n",
    "print(f\"\\nüìÅ File salvati:\")\n",
    "print(f\"   - lightgbm_quantile_model.txt\")\n",
    "print(f\"   - catboost_quantile_model.cbm\")\n",
    "print(f\"   - xgboost_quantile_model.json\")\n",
    "print(f\"   - best_nn_model.pth\")\n",
    "print(f\"   - feature_columns.pkl\")\n",
    "print(f\"   - scaler.pkl\")\n",
    "print(f\"   - ensemble_weights.pkl\")\n",
    "print(f\"   - metrics_summary.pkl\")\n",
    "\n",
    "print(f\"\\nüéâ Training completato con successo!\")\n",
    "print(f\"\\nüèÜ Miglior modello: Ensemble\")\n",
    "print(f\"   Quantile Loss (0.8): {ensemble_metrics_test['quantile_loss']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntnu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
