{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa3d15e",
   "metadata": {},
   "source": [
    "# 🎯 Generate Kaggle Submission File\n",
    "\n",
    "This notebook generates predictions for the Kaggle competition using the trained XGBoost model and the prediction_mapping.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f22be30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🚀 GENERATING KAGGLE SUBMISSION\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from xgboost import XGBRegressor\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"🚀 GENERATING KAGGLE SUBMISSION\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362635b0",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "967e69bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 Loading trained model and feature list...\n",
      "✅ Model loaded: XGBRegressor\n",
      "✅ Number of features: 29\n",
      "\n",
      "📋 Features: ['supplier_id', 'rm_id', 'product_id', 'year', 'month', 'day_of_week', 'quarter', 'is_weekend', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'days_since_start', 'supplier_mean_weight', 'supplier_median_weight', 'supplier_std_weight', 'supplier_cv', 'supplier_total_receivals', 'weight_lag_1', 'weight_lag_3_mean', 'weight_lag_7_mean', 'supplier_trend', 'rm_mean_weight', 'rm_std_weight', 'rm_count', 'product_mean_weight', 'product_std_weight', 'product_count', 'supplier_rm_frequency']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n📦 Loading trained model and feature list...\")\n",
    "\n",
    "# Load the trained XGBoost model\n",
    "with open('models/xgboost_baseline.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Load feature columns\n",
    "with open('models/feature_cols.pkl', 'rb') as f:\n",
    "    feature_cols = pickle.load(f)\n",
    "\n",
    "print(f\"✅ Model loaded: {type(model).__name__}\")\n",
    "print(f\"✅ Number of features: {len(feature_cols)}\")\n",
    "print(f\"\\n📋 Features: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5fe5de",
   "metadata": {},
   "source": [
    "## 2. Load Historical Training Data\n",
    "\n",
    "We need historical data to compute statistics and features for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c64e3fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "📊 Loading historical data...\n",
      "======================================================================\n",
      "✅ Receivals data: (122590, 10)\n",
      "✅ Completed receivals: (122448, 10)\n",
      "📅 Historical range: 2004-06-15 to 2024-12-19\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📊 Loading historical data...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load historical receivals data\n",
    "receivals = pd.read_csv('../data/kernel/receivals.csv')\n",
    "print(f\"✅ Receivals data: {receivals.shape}\")\n",
    "\n",
    "# Convert date to datetime\n",
    "receivals['date_arrival'] = pd.to_datetime(receivals['date_arrival'], format='%Y-%m-%d %H:%M:%S %z', utc=True, errors='coerce')\n",
    "\n",
    "# Filter only completed receivals (same as training)\n",
    "receivals = receivals[receivals['receival_status'] == 'Completed'].copy()\n",
    "print(f\"✅ Completed receivals: {receivals.shape}\")\n",
    "print(f\"📅 Historical range: {receivals['date_arrival'].min().date()} to {receivals['date_arrival'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e1cd66",
   "metadata": {},
   "source": [
    "## 3. Load Prediction Mapping\n",
    "\n",
    "This file contains the combinations of rm_id and date ranges we need to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "578a544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🎯 Loading prediction mapping...\n",
      "======================================================================\n",
      "✅ Prediction mapping: (30450, 4)\n",
      "\n",
      "📄 First few rows:\n",
      "   ID  rm_id forecast_start_date forecast_end_date\n",
      "0   1    365          2025-01-01        2025-01-02\n",
      "1   2    365          2025-01-01        2025-01-03\n",
      "2   3    365          2025-01-01        2025-01-04\n",
      "3   4    365          2025-01-01        2025-01-05\n",
      "4   5    365          2025-01-01        2025-01-06\n",
      "5   6    365          2025-01-01        2025-01-07\n",
      "6   7    365          2025-01-01        2025-01-08\n",
      "7   8    365          2025-01-01        2025-01-09\n",
      "8   9    365          2025-01-01        2025-01-10\n",
      "9  10    365          2025-01-01        2025-01-11\n",
      "\n",
      "📅 Forecast range: 2025-01-01 to 2025-05-31\n",
      "🔢 Unique rm_ids: 203\n",
      "✅ Total predictions needed: 30450\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🎯 Loading prediction mapping...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load prediction mapping\n",
    "pred_mapping = pd.read_csv('../data/prediction_mapping.csv')\n",
    "print(f\"✅ Prediction mapping: {pred_mapping.shape}\")\n",
    "print(f\"\\n📄 First few rows:\")\n",
    "print(pred_mapping.head(10))\n",
    "\n",
    "# Convert dates\n",
    "pred_mapping['forecast_start_date'] = pd.to_datetime(pred_mapping['forecast_start_date'])\n",
    "pred_mapping['forecast_end_date'] = pd.to_datetime(pred_mapping['forecast_end_date'])\n",
    "\n",
    "print(f\"\\n📅 Forecast range: {pred_mapping['forecast_start_date'].min().date()} to {pred_mapping['forecast_end_date'].max().date()}\")\n",
    "print(f\"🔢 Unique rm_ids: {pred_mapping['rm_id'].nunique()}\")\n",
    "print(f\"✅ Total predictions needed: {len(pred_mapping)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92e9998",
   "metadata": {},
   "source": [
    "## 4. Compute Historical Statistics\n",
    "\n",
    "Calculate statistics from historical data that will be used as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce0fde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "📈 Computing historical statistics...\n",
      "======================================================================\n",
      "✅ Supplier statistics computed: 255 suppliers\n",
      "✅ RM statistics computed: 203 materials\n",
      "✅ Product statistics computed: 54 products\n",
      "✅ Supplier-RM frequencies computed\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📈 Computing historical statistics...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Supplier statistics\n",
    "supplier_stats = receivals.groupby('supplier_id')['net_weight'].agg([\n",
    "    ('mean', 'mean'),\n",
    "    ('median', 'median'),\n",
    "    ('std', 'std'),\n",
    "    ('count', 'count')\n",
    "]).reset_index()\n",
    "supplier_stats.columns = ['supplier_id', 'supplier_mean_weight', 'supplier_median_weight', 'supplier_std_weight', 'supplier_total_receivals']\n",
    "supplier_stats['supplier_cv'] = supplier_stats['supplier_std_weight'] / supplier_stats['supplier_mean_weight']\n",
    "supplier_stats = supplier_stats.fillna(0)\n",
    "\n",
    "print(f\"✅ Supplier statistics computed: {len(supplier_stats)} suppliers\")\n",
    "\n",
    "# RM statistics\n",
    "rm_stats = receivals.groupby('rm_id')['net_weight'].agg(['mean', 'std', 'count']).reset_index()\n",
    "rm_stats.columns = ['rm_id', 'rm_mean_weight', 'rm_std_weight', 'rm_count']\n",
    "rm_stats = rm_stats.fillna(0)\n",
    "\n",
    "print(f\"✅ RM statistics computed: {len(rm_stats)} materials\")\n",
    "\n",
    "# Product statistics\n",
    "product_stats = receivals.groupby('product_id')['net_weight'].agg(['mean', 'std', 'count']).reset_index()\n",
    "product_stats.columns = ['product_id', 'product_mean_weight', 'product_std_weight', 'product_count']\n",
    "product_stats = product_stats.fillna(0)\n",
    "\n",
    "print(f\"✅ Product statistics computed: {len(product_stats)} products\")\n",
    "\n",
    "# Supplier-RM frequency\n",
    "supplier_rm_freq = receivals.groupby(['supplier_id', 'rm_id']).size().reset_index(name='supplier_rm_frequency')\n",
    "\n",
    "print(f\"✅ Supplier-RM frequencies computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff89eb",
   "metadata": {},
   "source": [
    "## 5. Get Most Recent Supplier for Each RM\n",
    "\n",
    "For prediction, we need to associate each rm_id with a supplier. We'll use the most recent supplier for each RM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033d8743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🔗 Mapping RM to most recent suppliers...\n",
      "======================================================================\n",
      "✅ Mapped 203 materials to suppliers\n",
      "\n",
      "📄 Sample mappings:\n",
      "   rm_id  supplier_id  product_id\n",
      "0  342.0        52433  91900170.0\n",
      "1  343.0        54748  91900143.0\n",
      "2  345.0        50387  91900143.0\n",
      "3  346.0        20023  91900146.0\n",
      "4  347.0        52064  91900143.0\n",
      "5  348.0        50387  91900143.0\n",
      "6  353.0        50390  91900143.0\n",
      "7  354.0        54764  91900182.0\n",
      "8  355.0        10001  91900152.0\n",
      "9  357.0        10001  91900152.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🔗 Mapping RM to most recent suppliers...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Sort by date and get most recent supplier for each rm_id\n",
    "receivals_sorted = receivals.sort_values(['rm_id', 'date_arrival'], ascending=[True, False])\n",
    "rm_to_supplier = receivals_sorted.groupby('rm_id').first()[['supplier_id', 'product_id']].reset_index()\n",
    "\n",
    "print(f\"✅ Mapped {len(rm_to_supplier)} materials to suppliers\")\n",
    "print(f\"\\n📄 Sample mappings:\")\n",
    "print(rm_to_supplier.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f442cc4",
   "metadata": {},
   "source": [
    "## 6. Prepare Prediction Dataset\n",
    "\n",
    "Create features for each prediction row using the same logic as training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e4c1cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🔧 Preparing prediction dataset...\n",
      "======================================================================\n",
      "✅ Base prediction data: (30450, 7)\n",
      "\n",
      "📄 Sample:\n",
      "   ID  rm_id forecast_start_date forecast_end_date  supplier_id  product_id  \\\n",
      "0   1    365          2025-01-01        2025-01-02        50387  91900143.0   \n",
      "1   2    365          2025-01-01        2025-01-03        50387  91900143.0   \n",
      "2   3    365          2025-01-01        2025-01-04        50387  91900143.0   \n",
      "3   4    365          2025-01-01        2025-01-05        50387  91900143.0   \n",
      "4   5    365          2025-01-01        2025-01-06        50387  91900143.0   \n",
      "\n",
      "  date_arrival  \n",
      "0   2025-01-02  \n",
      "1   2025-01-03  \n",
      "2   2025-01-04  \n",
      "3   2025-01-05  \n",
      "4   2025-01-06  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🔧 Preparing prediction dataset...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Merge prediction mapping with supplier info\n",
    "pred_data = pred_mapping.merge(rm_to_supplier, on='rm_id', how='left')\n",
    "\n",
    "# For missing suppliers, use the most common supplier for that RM or a default\n",
    "missing_supplier_mask = pred_data['supplier_id'].isna()\n",
    "if missing_supplier_mask.sum() > 0:\n",
    "    print(f\"⚠️  {missing_supplier_mask.sum()} rows without supplier mapping\")\n",
    "    # Use mode of supplier_id or default to first supplier\n",
    "    default_supplier = receivals['supplier_id'].mode()[0] if len(receivals) > 0 else 1\n",
    "    pred_data.loc[missing_supplier_mask, 'supplier_id'] = default_supplier\n",
    "    default_product = receivals['product_id'].mode()[0] if len(receivals) > 0 else 1\n",
    "    pred_data.loc[missing_supplier_mask, 'product_id'] = default_product\n",
    "    print(f\"   → Filled with default supplier: {default_supplier}\")\n",
    "\n",
    "# Use forecast_end_date as the target date for prediction\n",
    "pred_data['date_arrival'] = pred_data['forecast_end_date']\n",
    "\n",
    "print(f\"✅ Base prediction data: {pred_data.shape}\")\n",
    "print(f\"\\n📄 Sample:\")\n",
    "print(pred_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2becaa",
   "metadata": {},
   "source": [
    "## 7. Engineer Features for Prediction\n",
    "\n",
    "Create all the same features used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b931e9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "⚙️  Engineering features...\n",
      "======================================================================\n",
      "✅ Temporal features created\n",
      "✅ Historical statistics merged\n",
      "✅ Lag and trend features estimated\n",
      "✅ All features ready: (30450, 60)\n",
      "\n",
      "📊 Feature summary:\n",
      "        supplier_id         rm_id    product_id     year         month  \\\n",
      "count  30450.000000  30450.000000  3.045000e+04  30450.0  30450.000000   \n",
      "mean   56414.142857   2345.359606  8.148843e+07   2025.0      3.026667   \n",
      "std    18093.144406   1124.596786  2.912886e+07      0.0      1.418692   \n",
      "min    10001.000000    342.000000  1.002000e+03   2025.0      1.000000   \n",
      "25%    50420.000000   1875.000000  9.190015e+07   2025.0      2.000000   \n",
      "50%    55251.000000   2159.000000  9.190030e+07   2025.0      3.000000   \n",
      "75%    69879.000000   3142.000000  9.190120e+07   2025.0      4.000000   \n",
      "max    89313.000000   4501.000000  9.190209e+07   2025.0      5.000000   \n",
      "\n",
      "        day_of_week       quarter    is_weekend     month_sin     month_cos  \\\n",
      "count  30450.000000  30450.000000  30450.000000  30450.000000  3.045000e+04   \n",
      "mean       3.020000      1.406667      0.286667      0.744863 -1.244017e-02   \n",
      "std        1.988231      0.491220      0.452212      0.208599  6.336601e-01   \n",
      "min        0.000000      1.000000      0.000000      0.500000 -8.660254e-01   \n",
      "25%        1.000000      1.000000      0.000000      0.500000 -5.000000e-01   \n",
      "50%        3.000000      1.000000      0.000000      0.866025  6.123234e-17   \n",
      "75%        5.000000      2.000000      1.000000      0.866025  5.000000e-01   \n",
      "max        6.000000      2.000000      1.000000      1.000000  8.660254e-01   \n",
      "\n",
      "       ...  weight_lag_3_mean  weight_lag_7_mean  supplier_trend  \\\n",
      "count  ...       30450.000000       30450.000000         30450.0   \n",
      "mean   ...        6741.912030        6741.912030             1.0   \n",
      "std    ...        8997.910467        8997.910467             0.0   \n",
      "min    ...           0.000000           0.000000             1.0   \n",
      "25%    ...           0.000000           0.000000             1.0   \n",
      "50%    ...           0.000000           0.000000             1.0   \n",
      "75%    ...       11170.000000       11170.000000             1.0   \n",
      "max    ...       25181.428571       25181.428571             1.0   \n",
      "\n",
      "       rm_mean_weight  rm_std_weight      rm_count  product_mean_weight  \\\n",
      "count    30450.000000   30450.000000  30450.000000         30450.000000   \n",
      "mean     13694.747335    4133.689942    603.142857         13162.896692   \n",
      "std       8077.228031    3226.930836   2420.635838          7205.509151   \n",
      "min        597.350000       0.000000      1.000000           673.024390   \n",
      "25%       6839.218325     721.248917      3.000000          5895.226816   \n",
      "50%      13355.000000    3940.192619     21.000000         13871.723360   \n",
      "75%      22550.579646    6733.318121    183.000000         19940.260870   \n",
      "max      26028.750000   13371.851891  23608.000000         24738.023206   \n",
      "\n",
      "       product_std_weight  product_count  supplier_rm_frequency  \n",
      "count        30450.000000   30450.000000           30450.000000  \n",
      "mean          5548.885741    8941.591133             136.679803  \n",
      "std           2438.269785   17954.087991             505.300878  \n",
      "min             29.393877       2.000000               1.000000  \n",
      "25%           4291.870435      64.000000               1.000000  \n",
      "50%           6295.461884    4264.000000               6.000000  \n",
      "75%           7879.378056    6249.000000              47.000000  \n",
      "max           9176.396859   62500.000000            5314.000000  \n",
      "\n",
      "[8 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"⚙️  Engineering features...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Temporal features\n",
    "pred_data['year'] = pred_data['date_arrival'].dt.year\n",
    "pred_data['month'] = pred_data['date_arrival'].dt.month\n",
    "pred_data['day_of_week'] = pred_data['date_arrival'].dt.dayofweek\n",
    "pred_data['quarter'] = pred_data['date_arrival'].dt.quarter\n",
    "pred_data['is_weekend'] = pred_data['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Cyclical encoding\n",
    "pred_data['month_sin'] = np.sin(2 * np.pi * pred_data['month'] / 12)\n",
    "pred_data['month_cos'] = np.cos(2 * np.pi * pred_data['month'] / 12)\n",
    "pred_data['day_sin'] = np.sin(2 * np.pi * pred_data['day_of_week'] / 7)\n",
    "pred_data['day_cos'] = np.cos(2 * np.pi * pred_data['day_of_week'] / 7)\n",
    "\n",
    "# Days since start (use historical start as reference)\n",
    "historical_start = receivals['date_arrival'].min()\n",
    "# Localize prediction dates to UTC to match historical data timezone\n",
    "pred_data_arrival_utc = pd.to_datetime(pred_data['date_arrival']).dt.tz_localize('UTC')\n",
    "pred_data['days_since_start'] = (pred_data_arrival_utc - historical_start).dt.days\n",
    "\n",
    "print(f\"✅ Temporal features created\")\n",
    "\n",
    "# Merge with historical statistics\n",
    "pred_data = pred_data.merge(supplier_stats, on='supplier_id', how='left')\n",
    "pred_data = pred_data.merge(rm_stats, on='rm_id', how='left')\n",
    "pred_data = pred_data.merge(product_stats, on='product_id', how='left')\n",
    "pred_data = pred_data.merge(supplier_rm_freq, on=['supplier_id', 'rm_id'], how='left')\n",
    "\n",
    "print(f\"✅ Historical statistics merged\")\n",
    "\n",
    "# Lag features - use historical averages as proxies since we don't have real lag data for future\n",
    "# For each supplier, get recent average weights\n",
    "recent_window = 30  # last 30 days\n",
    "cutoff_date = receivals['date_arrival'].max() - pd.Timedelta(days=recent_window)\n",
    "recent_data = receivals[receivals['date_arrival'] > cutoff_date]\n",
    "\n",
    "# Recent averages by supplier\n",
    "recent_supplier_avg = recent_data.groupby('supplier_id')['net_weight'].mean().reset_index()\n",
    "recent_supplier_avg.columns = ['supplier_id', 'recent_avg_weight']\n",
    "pred_data = pred_data.merge(recent_supplier_avg, on='supplier_id', how='left')\n",
    "\n",
    "# Use recent average as proxy for lag features\n",
    "pred_data['weight_lag_1'] = pred_data['recent_avg_weight_x']\n",
    "pred_data['weight_lag_3_mean'] = pred_data['recent_avg_weight_x']\n",
    "pred_data['weight_lag_7_mean'] = pred_data['recent_avg_weight_x']\n",
    "\n",
    "# Supplier trend - assume stable (1.0) for future predictions\n",
    "pred_data['supplier_trend'] = 1.0\n",
    "\n",
    "print(f\"✅ Lag and trend features estimated\")\n",
    "\n",
    "# Fill any remaining NaN with 0\n",
    "pred_data = pred_data.fillna(0)\n",
    "\n",
    "print(f\"✅ All features ready: {pred_data.shape}\")\n",
    "print(f\"\\n📊 Feature summary:\")\n",
    "print(pred_data[feature_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a91b5e",
   "metadata": {},
   "source": [
    "## 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d192477a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🔮 Making predictions...\n",
      "======================================================================\n",
      "✅ Feature matrix prepared: (30450, 29)\n",
      "📊 Feature matrix info:\n",
      "   - Total features: 29\n",
      "   - Total samples: 30450\n",
      "   - Missing values: 0\n",
      "\n",
      "✅ Predictions generated: 30450 values\n",
      "⚠️  Clipped 188 negative predictions to 0 (0.62%)\n",
      "\n",
      "📊 Prediction statistics:\n",
      "   Mean:     12482.85 kg\n",
      "   Median:   13961.54 kg\n",
      "   Std Dev:  6208.74 kg\n",
      "   Min:      0.00 kg\n",
      "   Max:      24913.39 kg\n",
      "   Q1 (25%): 6893.93 kg\n",
      "   Q3 (75%): 17869.76 kg\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🔮 Making predictions...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Ensure all required features are present\n",
    "missing_features = [f for f in feature_cols if f not in pred_data.columns]\n",
    "if missing_features:\n",
    "    print(f\"❌ Missing features: {missing_features}\")\n",
    "    raise ValueError(f\"Cannot proceed: missing features {missing_features}\")\n",
    "\n",
    "# Prepare feature matrix\n",
    "X_pred = pred_data[feature_cols]\n",
    "\n",
    "print(f\"✅ Feature matrix prepared: {X_pred.shape}\")\n",
    "print(f\"📊 Feature matrix info:\")\n",
    "print(f\"   - Total features: {X_pred.shape[1]}\")\n",
    "print(f\"   - Total samples: {X_pred.shape[0]}\")\n",
    "print(f\"   - Missing values: {X_pred.isna().sum().sum()}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_pred)\n",
    "\n",
    "# Clip negative predictions to 0\n",
    "num_negative = (predictions < 0).sum()\n",
    "predictions = np.clip(predictions, 0, None)\n",
    "\n",
    "print(f\"\\n✅ Predictions generated: {len(predictions)} values\")\n",
    "\n",
    "if num_negative > 0:\n",
    "    print(f\"⚠️  Clipped {num_negative} negative predictions to 0 ({num_negative/len(predictions)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n📊 Prediction statistics:\")\n",
    "print(f\"   Mean:     {predictions.mean():.2f} kg\")\n",
    "print(f\"   Median:   {np.median(predictions):.2f} kg\")\n",
    "print(f\"   Std Dev:  {predictions.std():.2f} kg\")\n",
    "print(f\"   Min:      {predictions.min():.2f} kg\")\n",
    "print(f\"   Max:      {predictions.max():.2f} kg\")\n",
    "print(f\"   Q1 (25%): {np.percentile(predictions, 25):.2f} kg\")\n",
    "print(f\"   Q3 (75%): {np.percentile(predictions, 75):.2f} kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d6494",
   "metadata": {},
   "source": [
    "## 9. Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85bbbeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "📝 Creating submission file...\n",
      "======================================================================\n",
      "\n",
      "✅ Submission dataframe created: (30450, 2)\n",
      "\n",
      "📄 First 20 rows:\n",
      "    ID  predicted_weight\n",
      "0    1      16930.419922\n",
      "1    2      17749.080078\n",
      "2    3      17360.759766\n",
      "3    4      14337.480469\n",
      "4    5      17600.640625\n",
      "5    6      17329.759766\n",
      "6    7      17329.759766\n",
      "7    8      16930.419922\n",
      "8    9      17749.080078\n",
      "9   10      17360.759766\n",
      "10  11      14337.480469\n",
      "11  12      17600.640625\n",
      "12  13      17329.759766\n",
      "13  14      17329.759766\n",
      "14  15      16930.419922\n",
      "15  16      17749.080078\n",
      "16  17      17360.759766\n",
      "17  18      14337.480469\n",
      "18  19      17600.640625\n",
      "19  20      17329.759766\n",
      "\n",
      "✅ All validations passed!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📝 Creating submission file...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'ID': pred_mapping['ID'],\n",
    "    'predicted_weight': predictions\n",
    "})\n",
    "\n",
    "# Round to 2 decimal places\n",
    "submission['predicted_weight'] = submission['predicted_weight'].round(2)\n",
    "\n",
    "# Verify format\n",
    "print(f\"\\n✅ Submission dataframe created: {submission.shape}\")\n",
    "print(f\"\\n📄 First 20 rows:\")\n",
    "print(submission.head(20))\n",
    "\n",
    "# Validate\n",
    "assert list(submission.columns) == ['ID', 'predicted_weight'], \"❌ Column names incorrect\"\n",
    "assert len(submission) == 30450, f\"❌ Wrong number of rows: {len(submission)} (expected 30450)\"\n",
    "assert submission['ID'].iloc[0] == 1, \"❌ ID should start from 1\"\n",
    "assert submission['ID'].iloc[-1] == 30450, f\"❌ Last ID should be 30450, got {submission['ID'].iloc[-1]}\"\n",
    "assert submission['ID'].is_monotonic_increasing, \"❌ IDs should be sequential\"\n",
    "assert not submission['predicted_weight'].isna().any(), \"❌ Predictions contain NaN\"\n",
    "\n",
    "print(f\"\\n✅ All validations passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b1028",
   "metadata": {},
   "source": [
    "## 10. Save Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "550b2863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "💾 Saving submission file...\n",
      "======================================================================\n",
      "✅ Submission saved to: submission.csv\n",
      "✅ File size: 430,144 bytes (420.06 KB)\n",
      "✅ Number of lines in file: 30,451 (including header)\n",
      "✅ PERFECT! File has exactly 30451 lines (30450 predictions + header)\n",
      "\n",
      "📋 Comparing with sample_submission.csv...\n",
      "   Sample shape: (30450, 2)\n",
      "   Our shape:    (30450, 2)\n",
      "✅ Shapes match perfectly!\n",
      "\n",
      "======================================================================\n",
      "🎉 SUBMISSION FILE READY FOR KAGGLE!\n",
      "======================================================================\n",
      "\n",
      "📁 File: submission.csv\n",
      "📊 Rows: 30,450 predictions\n",
      "✅ Format: Validated\n",
      "🚀 Status: Ready to upload!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"💾 Saving submission file...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = 'submission.csv'\n",
    "submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Submission saved to: {output_path}\")\n",
    "\n",
    "# Verify file\n",
    "import os\n",
    "file_size = os.path.getsize(output_path)\n",
    "print(f\"✅ File size: {file_size:,} bytes ({file_size/1024:.2f} KB)\")\n",
    "\n",
    "# Count lines in file\n",
    "with open(output_path, 'r') as f:\n",
    "    num_lines = sum(1 for _ in f)\n",
    "\n",
    "print(f\"✅ Number of lines in file: {num_lines:,} (including header)\")\n",
    "\n",
    "if num_lines == 30451:  # 30450 predictions + 1 header\n",
    "    print(f\"✅ PERFECT! File has exactly 30451 lines (30450 predictions + header)\")\n",
    "else:\n",
    "    print(f\"⚠️  WARNING: Expected 30451 lines, got {num_lines}\")\n",
    "\n",
    "# Load sample submission and compare format\n",
    "print(f\"\\n📋 Comparing with sample_submission.csv...\")\n",
    "sample = pd.read_csv('../data/sample_submission.csv')\n",
    "print(f\"   Sample shape: {sample.shape}\")\n",
    "print(f\"   Our shape:    {submission.shape}\")\n",
    "\n",
    "if sample.shape == submission.shape:\n",
    "    print(f\"✅ Shapes match perfectly!\")\n",
    "else:\n",
    "    print(f\"⚠️  Shape mismatch!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🎉 SUBMISSION FILE READY FOR KAGGLE!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n📁 File: {output_path}\")\n",
    "print(f\"📊 Rows: {len(submission):,} predictions\")\n",
    "print(f\"✅ Format: Validated\")\n",
    "print(f\"🚀 Status: Ready to upload!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682c3bf8",
   "metadata": {},
   "source": [
    "## 11. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e25bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📊 FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n🎯 Prediction Distribution by RM:\")\n",
    "rm_pred_summary = pred_data[['rm_id']].copy()\n",
    "rm_pred_summary['predicted_weight'] = predictions\n",
    "rm_summary = rm_pred_summary.groupby('rm_id')['predicted_weight'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "rm_summary = rm_summary.sort_values('count', ascending=False)\n",
    "print(rm_summary.head(10))\n",
    "\n",
    "print(f\"\\n📅 Prediction Distribution by Month:\")\n",
    "pred_data['pred_month'] = pred_data['date_arrival'].dt.to_period('M')\n",
    "monthly_summary = pred_data.groupby('pred_month').size()\n",
    "print(monthly_summary)\n",
    "\n",
    "print(f\"\\n✅ Submission generation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntnu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
