{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c2c86d",
   "metadata": {},
   "source": [
    "# Feature Engineering - Versione Integrata\n",
    "\n",
    "**Approccio unificato che combina:**\n",
    "- ✅ **Tuo progetto**: Logica domain-specific, feature temporali avanzate\n",
    "- ✅ **Repository riferimento**: 48+ features, finestre temporali multiple, lag/trend/EWM\n",
    "\n",
    "**Pipeline**: data_clean.csv → Advanced features → train_data.csv & test_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e8d61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Feature Engineering - Versione Integrata\n",
      "   Combinando il meglio del tuo progetto + repository riferimento\n",
      "\n",
      "📦 Dataset aggregato caricato: (41907, 9)\n",
      "📋 Colonne disponibili: ['date_arrival', 'rm_id', 'net_weight', 'num_deliveries', 'supplier_id', 'product_id', 'quantity', 'delivery_delay_days', 'receival_status']\n",
      "\n",
      "✅ Verifica aggregazione giornaliera:\n",
      "   Range date: 2004-06-15 00:00:00 → 2024-12-19 00:00:00\n",
      "   Materiali unici: 204\n",
      "   Record totali: 41907\n",
      "✅ Dataset ordinato per rolling window calculations\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"🚀 Feature Engineering - Versione Integrata\")\n",
    "print(\"   Combinando il meglio del tuo progetto + repository riferimento\")\n",
    "\n",
    "# Carica il dataset aggregato giornalmente da data_cleaning.ipynb\n",
    "data_clean = pd.read_csv('data_clean.csv')\n",
    "\n",
    "print(f\"\\n📦 Dataset aggregato caricato: {data_clean.shape}\")\n",
    "print(f\"📋 Colonne disponibili: {list(data_clean.columns)}\")\n",
    "\n",
    "# Converti date_arrival e verifica aggregazione\n",
    "data_clean['date_arrival'] = pd.to_datetime(data_clean['date_arrival'])\n",
    "print(f\"\\n✅ Verifica aggregazione giornaliera:\")\n",
    "print(f\"   Range date: {data_clean['date_arrival'].min()} → {data_clean['date_arrival'].max()}\")\n",
    "print(f\"   Materiali unici: {data_clean['rm_id'].nunique()}\")\n",
    "print(f\"   Record totali: {len(data_clean)}\")\n",
    "\n",
    "# Ordina per rm_id e data per calcoli rolling corretti\n",
    "data_clean = data_clean.sort_values(['rm_id', 'date_arrival']).reset_index(drop=True)\n",
    "print(f\"✅ Dataset ordinato per rolling window calculations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0920cf90",
   "metadata": {},
   "source": [
    "## 1. Temporal Window Features (Repository Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a85fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Calcolo Temporal Window Features...\n",
      "   📊 Finestra 7 giorni...\n",
      "   📊 Finestra 30 giorni...\n",
      "   📊 Finestra 90 giorni...\n",
      "✅ Temporal Window Features: 15 = 15 features\n",
      "\n",
      "🔍 Esempio features (primi 5 record):\n",
      "  date_arrival  rm_id  net_weight  weight_mean_7d  weight_mean_30d  weight_mean_90d\n",
      "0   2022-12-16   -1.0     42132.0         42132.0          42132.0          42132.0\n",
      "1   2004-06-23  342.0     24940.0         24940.0          24940.0          24940.0\n",
      "2   2005-03-29  343.0     21760.0         21760.0          21760.0          21760.0\n",
      "3   2004-09-01  345.0     22780.0         22780.0          22780.0          22780.0\n",
      "4   2004-06-24  346.0       820.0           820.0            820.0            820.0\n"
     ]
    }
   ],
   "source": [
    "# FINESTRE TEMPORALI: [7, 30, 90] giorni (ottimizzato dal repository)\n",
    "# Per ogni finestra calcoliamo: sum, mean, std, max, count\n",
    "\n",
    "print(\"🔄 Calcolo Temporal Window Features...\")\n",
    "\n",
    "# Finestre temporali essenziali (3 invece di 8 per efficienza)\n",
    "windows = [7, 30, 90]\n",
    "\n",
    "for window in windows:\n",
    "    print(f\"   📊 Finestra {window} giorni...\")\n",
    "    \n",
    "    # Raggruppa per material e calcola rolling statistics\n",
    "    grouped = data_clean.groupby('rm_id')\n",
    "    \n",
    "    data_clean[f'weight_sum_{window}d'] = (\n",
    "        grouped['net_weight'].rolling(window=window, min_periods=1).sum().reset_index(0, drop=True)\n",
    "    )\n",
    "    \n",
    "    data_clean[f'weight_mean_{window}d'] = (\n",
    "        grouped['net_weight'].rolling(window=window, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    )\n",
    "    \n",
    "    data_clean[f'weight_std_{window}d'] = (\n",
    "        grouped['net_weight'].rolling(window=window, min_periods=1).std().reset_index(0, drop=True)\n",
    "    ).fillna(0)\n",
    "    \n",
    "    data_clean[f'weight_max_{window}d'] = (\n",
    "        grouped['net_weight'].rolling(window=window, min_periods=1).max().reset_index(0, drop=True)\n",
    "    )\n",
    "    \n",
    "    data_clean[f'deliveries_sum_{window}d'] = (\n",
    "        grouped['num_deliveries'].rolling(window=window, min_periods=1).sum().reset_index(0, drop=True)\n",
    "    )\n",
    "\n",
    "print(f\"✅ Temporal Window Features: {len(windows) * 5} = {len(windows) * 5} features\")\n",
    "\n",
    "# Verifica risultati\n",
    "sample_cols = ['date_arrival', 'rm_id', 'net_weight', 'weight_mean_7d', 'weight_mean_30d', 'weight_mean_90d']\n",
    "print(f\"\\n🔍 Esempio features (primi 5 record):\")\n",
    "print(data_clean[sample_cols].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a64451",
   "metadata": {},
   "source": [
    "## 2. Lag Features & Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aab01f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Calcolo Lag Features...\n",
      "🔄 Calcolo Ratio & Trend Features...\n",
      "✅ Lag & Trend Features: 8 features\n",
      "   - 2 lag features (7d, 30d)\n",
      "   - 2 ratio features (trend strength)\n",
      "   - 2 trend features (direction)\n",
      "   - 2 volatility features (stability)\n"
     ]
    }
   ],
   "source": [
    "# LAG FEATURES: Valori storici per pattern temporali\n",
    "print(\"🔄 Calcolo Lag Features...\")\n",
    "\n",
    "# Lag features (valori precedenti per ogni material)\n",
    "data_clean['weight_lag_7d'] = (\n",
    "    data_clean.groupby('rm_id')['net_weight'].shift(7).fillna(0)\n",
    ")\n",
    "\n",
    "data_clean['weight_lag_30d'] = (\n",
    "    data_clean.groupby('rm_id')['net_weight'].shift(30).fillna(0)\n",
    ")\n",
    "\n",
    "# RATIO FEATURES: Confronti tra periodi diversi\n",
    "print(\"🔄 Calcolo Ratio & Trend Features...\")\n",
    "\n",
    "# Ratio tra finestre (attività recente vs storica)\n",
    "data_clean['ratio_7d_30d'] = (\n",
    "    data_clean['weight_mean_7d'] / (data_clean['weight_mean_30d'] + 1e-6)\n",
    ").fillna(1)\n",
    "\n",
    "data_clean['ratio_30d_90d'] = (\n",
    "    data_clean['weight_mean_30d'] / (data_clean['weight_mean_90d'] + 1e-6)\n",
    ").fillna(1)\n",
    "\n",
    "# Trend features (cambiamenti assoluti)\n",
    "data_clean['trend_7d_30d'] = (\n",
    "    data_clean['weight_mean_7d'] - data_clean['weight_mean_30d']\n",
    ").fillna(0)\n",
    "\n",
    "data_clean['trend_30d_90d'] = (\n",
    "    data_clean['weight_mean_30d'] - data_clean['weight_mean_90d']\n",
    ").fillna(0)\n",
    "\n",
    "# Coefficient of variation (volatilità relativa)\n",
    "data_clean['cv_7d'] = (\n",
    "    data_clean['weight_std_7d'] / (data_clean['weight_mean_7d'] + 1e-6)\n",
    ").fillna(0)\n",
    "\n",
    "data_clean['cv_30d'] = (\n",
    "    data_clean['weight_std_30d'] / (data_clean['weight_mean_30d'] + 1e-6)\n",
    ").fillna(0)\n",
    "\n",
    "print(f\"✅ Lag & Trend Features: 8 features\")\n",
    "print(f\"   - 2 lag features (7d, 30d)\")\n",
    "print(f\"   - 2 ratio features (trend strength)\")\n",
    "print(f\"   - 2 trend features (direction)\")\n",
    "print(f\"   - 2 volatility features (stability)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a70710",
   "metadata": {},
   "source": [
    "## 3. EWM (Exponential Weighted Moving) Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b05fe4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Calcolo EWM Features...\n",
      "✅ EWM Features: 5 = 5 features\n",
      "   - EWM means e std per spans [7, 30]\n",
      "   - EWM ratio per trend detection\n"
     ]
    }
   ],
   "source": [
    "# EWM FEATURES: Medie ponderate che danno più peso ai valori recenti\n",
    "print(\"🔄 Calcolo EWM Features...\")\n",
    "\n",
    "# EWM features con diversi span (peso decrescente per valori più vecchi)\n",
    "spans = [7, 30]  # Due spans essenziali\n",
    "\n",
    "for span in spans:\n",
    "    data_clean[f'ewm_mean_{span}d'] = (\n",
    "        data_clean.groupby('rm_id')['net_weight']\n",
    "        .ewm(span=span, adjust=False).mean()\n",
    "        .reset_index(0, drop=True)\n",
    "    )\n",
    "    \n",
    "    data_clean[f'ewm_std_{span}d'] = (\n",
    "        data_clean.groupby('rm_id')['net_weight']\n",
    "        .ewm(span=span, adjust=False).std()\n",
    "        .reset_index(0, drop=True)\n",
    "    ).fillna(0)\n",
    "\n",
    "# EWM ratio (confronto tra diversi decay rates)\n",
    "data_clean['ewm_ratio_7d_30d'] = (\n",
    "    data_clean['ewm_mean_7d'] / (data_clean['ewm_mean_30d'] + 1e-6)\n",
    ").fillna(1)\n",
    "\n",
    "print(f\"✅ EWM Features: {len(spans) * 2 + 1} = 5 features\")\n",
    "print(f\"   - EWM means e std per spans {spans}\")\n",
    "print(f\"   - EWM ratio per trend detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d7b3d",
   "metadata": {},
   "source": [
    "## 4. Calendar & Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac247e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Calcolo Calendar Features...\n",
      "✅ Calendar Features: 15 features\n",
      "   - Basic calendar: day_of_week, month, quarter, etc.\n",
      "   - Business indicators: weekday, month/quarter start/end\n",
      "   - Cyclical encoding: sin/cos for periodicity\n",
      "   - Global trend: days_since_start\n",
      "✅ Calendar Features: 15 features\n",
      "   - Basic calendar: day_of_week, month, quarter, etc.\n",
      "   - Business indicators: weekday, month/quarter start/end\n",
      "   - Cyclical encoding: sin/cos for periodicity\n",
      "   - Global trend: days_since_start\n"
     ]
    }
   ],
   "source": [
    "# CALENDAR FEATURES: Pattern temporali (stagionalità, giorni lavorativi, etc.)\n",
    "print(\"🔄 Calcolo Calendar Features...\")\n",
    "\n",
    "# Basic calendar features\n",
    "data_clean['day_of_week'] = data_clean['date_arrival'].dt.dayofweek\n",
    "data_clean['day_of_month'] = data_clean['date_arrival'].dt.day\n",
    "data_clean['month'] = data_clean['date_arrival'].dt.month\n",
    "data_clean['quarter'] = data_clean['date_arrival'].dt.quarter\n",
    "data_clean['week_of_year'] = data_clean['date_arrival'].dt.isocalendar().week\n",
    "\n",
    "# Business day indicators\n",
    "data_clean['is_weekday'] = (data_clean['day_of_week'] < 5).astype(int)\n",
    "data_clean['is_month_start'] = data_clean['date_arrival'].dt.is_month_start.astype(int)\n",
    "data_clean['is_month_end'] = data_clean['date_arrival'].dt.is_month_end.astype(int)\n",
    "data_clean['is_quarter_start'] = data_clean['date_arrival'].dt.is_quarter_start.astype(int)\n",
    "data_clean['is_quarter_end'] = data_clean['date_arrival'].dt.is_quarter_end.astype(int)\n",
    "\n",
    "# Cyclical encoding (importante per modelli tree-based)\n",
    "data_clean['day_of_week_sin'] = np.sin(2 * np.pi * data_clean['day_of_week'] / 7)\n",
    "data_clean['day_of_week_cos'] = np.cos(2 * np.pi * data_clean['day_of_week'] / 7)\n",
    "data_clean['month_sin'] = np.sin(2 * np.pi * data_clean['month'] / 12)\n",
    "data_clean['month_cos'] = np.cos(2 * np.pi * data_clean['month'] / 12)\n",
    "\n",
    "# Time since start (trend temporale globale)\n",
    "min_date = data_clean['date_arrival'].min()\n",
    "data_clean['days_since_start'] = (data_clean['date_arrival'] - min_date).dt.days\n",
    "\n",
    "print(f\"✅ Calendar Features: 15 features\")\n",
    "print(f\"   - Basic calendar: day_of_week, month, quarter, etc.\")\n",
    "print(f\"   - Business indicators: weekday, month/quarter start/end\")\n",
    "print(f\"   - Cyclical encoding: sin/cos for periodicity\")\n",
    "print(f\"   - Global trend: days_since_start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b34553",
   "metadata": {},
   "source": [
    "## 5. Material & Supplier Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a926c578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Calcolo Material & Supplier Features...\n",
      "✅ Material & Supplier Features: 9 features\n",
      "   - RM_ID: target_mean, target_std, frequency\n",
      "   - Supplier: target_mean, frequency, material_diversity\n",
      "   - Product: target_mean, frequency\n"
     ]
    }
   ],
   "source": [
    "# MATERIAL & SUPPLIER FEATURES: Encoding e aggregazioni\n",
    "print(\"🔄 Calcolo Material & Supplier Features...\")\n",
    "\n",
    "# Target encoding per categorical features (rm_id, supplier_id, product_id)\n",
    "# Usando media globale del target per ogni categoria\n",
    "\n",
    "# RM_ID encoding (molto importante)\n",
    "rm_target_mean = data_clean.groupby('rm_id')['net_weight'].mean().to_dict()\n",
    "rm_target_std = data_clean.groupby('rm_id')['net_weight'].std().fillna(0).to_dict()\n",
    "rm_target_count = data_clean.groupby('rm_id').size().to_dict()\n",
    "\n",
    "data_clean['rm_id_target_mean'] = data_clean['rm_id'].map(rm_target_mean)\n",
    "data_clean['rm_id_target_std'] = data_clean['rm_id'].map(rm_target_std)\n",
    "data_clean['rm_id_frequency'] = data_clean['rm_id'].map(rm_target_count)\n",
    "\n",
    "# SUPPLIER_ID encoding\n",
    "supplier_target_mean = data_clean.groupby('supplier_id')['net_weight'].mean().to_dict()\n",
    "supplier_target_count = data_clean.groupby('supplier_id').size().to_dict()\n",
    "\n",
    "data_clean['supplier_target_mean'] = data_clean['supplier_id'].map(supplier_target_mean)\n",
    "data_clean['supplier_frequency'] = data_clean['supplier_id'].map(supplier_target_count)\n",
    "\n",
    "# PRODUCT_ID encoding\n",
    "product_target_mean = data_clean.groupby('product_id')['net_weight'].mean().to_dict()\n",
    "product_target_count = data_clean.groupby('product_id').size().to_dict()\n",
    "\n",
    "data_clean['product_target_mean'] = data_clean['product_id'].map(product_target_mean)\n",
    "data_clean['product_frequency'] = data_clean['product_id'].map(product_target_count)\n",
    "\n",
    "# Material diversity per supplier (quanti materiali diversi gestisce)\n",
    "supplier_material_count = data_clean.groupby('supplier_id')['rm_id'].nunique().to_dict()\n",
    "data_clean['supplier_material_diversity'] = data_clean['supplier_id'].map(supplier_material_count)\n",
    "\n",
    "print(f\"✅ Material & Supplier Features: 9 features\")\n",
    "print(f\"   - RM_ID: target_mean, target_std, frequency\")\n",
    "print(f\"   - Supplier: target_mean, frequency, material_diversity\")\n",
    "print(f\"   - Product: target_mean, frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76157c94",
   "metadata": {},
   "source": [
    "## 6. Train/Test Split & Final Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e65d18f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Preparazione finale dataset...\n",
      "📊 Feature totali create: 54\n",
      "   Suddivisione:\n",
      "   - Temporal windows: 15 features (3 finestre × 5 stats)\n",
      "   - Lag & trend: 8 features\n",
      "   - EWM: 5 features\n",
      "   - Calendar: 15 features\n",
      "   - Material/Supplier: 9 features\n",
      "   - TOTALE: 52 = 52 features\n",
      "\n",
      "🔍 Missing values nel dataset finale:\n",
      "   ✅ Nessun missing value!\n",
      "\n",
      "📊 Split temporale completato:\n",
      "   Training: 33525 record (80.0%)\n",
      "   Test: 8382 record (20.0%)\n",
      "   Split date: 2022-03-01 00:00:00\n",
      "\n",
      "✅ Dataset salvati:\n",
      "   train_data.csv: (33525, 60)\n",
      "   test_data.csv: (8382, 60)\n",
      "\n",
      "🔍 Sample features (prime 5 righe training):\n",
      "      net_weight  num_deliveries    quantity  delivery_delay_days  weight_sum_7d  weight_mean_7d  weight_std_7d  weight_max_7d  deliveries_sum_7d  weight_sum_30d  weight_mean_30d\n",
      "1023      8680.0               1    350000.0               -199.0         8680.0          8680.0            0.0         8680.0                1.0          8680.0           8680.0\n",
      "627       6745.0               1    600000.0               -199.0         6745.0          6745.0            0.0         6745.0                1.0          6745.0           6745.0\n",
      "499      13500.0               1     75000.0                -15.0        13500.0         13500.0            0.0        13500.0                1.0         13500.0          13500.0\n",
      "361       3015.0               1    600000.0               -199.0         3015.0          3015.0            0.0         3015.0                1.0          3015.0           3015.0\n",
      "184      83784.0               7  16400000.0               -199.0        83784.0         83784.0            0.0        83784.0                7.0         83784.0          83784.0\n",
      "\n",
      "✅ Dataset salvati:\n",
      "   train_data.csv: (33525, 60)\n",
      "   test_data.csv: (8382, 60)\n",
      "\n",
      "🔍 Sample features (prime 5 righe training):\n",
      "      net_weight  num_deliveries    quantity  delivery_delay_days  weight_sum_7d  weight_mean_7d  weight_std_7d  weight_max_7d  deliveries_sum_7d  weight_sum_30d  weight_mean_30d\n",
      "1023      8680.0               1    350000.0               -199.0         8680.0          8680.0            0.0         8680.0                1.0          8680.0           8680.0\n",
      "627       6745.0               1    600000.0               -199.0         6745.0          6745.0            0.0         6745.0                1.0          6745.0           6745.0\n",
      "499      13500.0               1     75000.0                -15.0        13500.0         13500.0            0.0        13500.0                1.0         13500.0          13500.0\n",
      "361       3015.0               1    600000.0               -199.0         3015.0          3015.0            0.0         3015.0                1.0          3015.0           3015.0\n",
      "184      83784.0               7  16400000.0               -199.0        83784.0         83784.0            0.0        83784.0                7.0         83784.0          83784.0\n"
     ]
    }
   ],
   "source": [
    "# FINALIZZAZIONE DATASET per ML\n",
    "print(\"🔄 Preparazione finale dataset...\")\n",
    "\n",
    "# Rimuovi colonne non necessarie per ML (date, IDs originali)\n",
    "feature_columns = [col for col in data_clean.columns if col not in [\n",
    "    'date_arrival', 'rm_id', 'supplier_id', 'product_id', \n",
    "    'receival_status', 'net_weight'  # net_weight è il target\n",
    "]]\n",
    "\n",
    "print(f\"📊 Feature totali create: {len(feature_columns)}\")\n",
    "print(f\"   Suddivisione:\")\n",
    "print(f\"   - Temporal windows: 15 features (3 finestre × 5 stats)\")\n",
    "print(f\"   - Lag & trend: 8 features\")\n",
    "print(f\"   - EWM: 5 features\")\n",
    "print(f\"   - Calendar: 15 features\")\n",
    "print(f\"   - Material/Supplier: 9 features\")\n",
    "print(f\"   - TOTALE: {15+8+5+15+9} = 52 features\")\n",
    "\n",
    "# Verifica missing values finali\n",
    "print(f\"\\n🔍 Missing values nel dataset finale:\")\n",
    "missing_final = data_clean[feature_columns + ['net_weight']].isnull().sum()\n",
    "missing_final = missing_final[missing_final > 0]\n",
    "if len(missing_final) > 0:\n",
    "    print(missing_final)\n",
    "else:\n",
    "    print(\"   ✅ Nessun missing value!\")\n",
    "\n",
    "# Split temporale (ultimo 20% per test)\n",
    "# Importante: per time series usare split temporale, non random\n",
    "data_clean = data_clean.sort_values('date_arrival')\n",
    "split_point = int(len(data_clean) * 0.8)\n",
    "\n",
    "train_data = data_clean.iloc[:split_point].copy()\n",
    "test_data = data_clean.iloc[split_point:].copy()\n",
    "\n",
    "print(f\"\\n📊 Split temporale completato:\")\n",
    "print(f\"   Training: {len(train_data)} record ({len(train_data)/len(data_clean)*100:.1f}%)\")\n",
    "print(f\"   Test: {len(test_data)} record ({len(test_data)/len(data_clean)*100:.1f}%)\")\n",
    "print(f\"   Split date: {data_clean.iloc[split_point]['date_arrival']}\")\n",
    "\n",
    "# Salva i dataset finali\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "test_data.to_csv('test_data.csv', index=False)\n",
    "\n",
    "print(f\"\\n✅ Dataset salvati:\")\n",
    "print(f\"   train_data.csv: {train_data.shape}\")\n",
    "print(f\"   test_data.csv: {test_data.shape}\")\n",
    "\n",
    "# Mostra sample delle feature finali\n",
    "print(f\"\\n🔍 Sample features (prime 5 righe training):\")\n",
    "sample_features = ['net_weight'] + feature_columns[:10]  # Target + prime 10 features\n",
    "print(train_data[sample_features].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e0a86a",
   "metadata": {},
   "source": [
    "## ✅ Feature Engineering Completato - Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37b765c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "✅ FEATURE ENGINEERING COMPLETATO - VERSIONE INTEGRATA\n",
      "================================================================================\n",
      "\n",
      "🎯 APPROCCIO UNIFICATO IMPLEMENTATO:\n",
      "\n",
      "1. 📊 REPOSITORY RIFERIMENTO (Advanced Features):\n",
      "   ✅ Temporal Windows: [7, 30, 90] giorni → 15 features\n",
      "   ✅ Lag Features: storico a 7d, 30d → 2 features\n",
      "   ✅ Ratio & Trend: confronti temporali → 6 features  \n",
      "   ✅ EWM Features: medie ponderate → 5 features\n",
      "   ✅ Calendar Features: stagionalità completa → 15 features\n",
      "\n",
      "2. 📈 TUO PROGETTO (Domain Knowledge):\n",
      "   ✅ Material encoding: target encoding per rm_id\n",
      "   ✅ Supplier analysis: diversity e frequency\n",
      "   ✅ Product features: statistical encoding\n",
      "   ✅ Missing value strategy: domain-specific → 9 features\n",
      "\n",
      "📊 OUTPUT FINALE:\n",
      "   Files: train_data.csv, test_data.csv\n",
      "   Features: 52 advanced features per ML\n",
      "   Split: Temporale (80/20) per time series\n",
      "   Target: net_weight (daily aggregated)\n",
      "\n",
      "🚀 PROSSIMO STEP: baseline_model.ipynb\n",
      "   Input: train_data.csv, test_data.csv\n",
      "   Processo: CatBoost + Quantile Loss + Optuna\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"✅ FEATURE ENGINEERING COMPLETATO - VERSIONE INTEGRATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "🎯 APPROCCIO UNIFICATO IMPLEMENTATO:\n",
    "\n",
    "1. 📊 REPOSITORY RIFERIMENTO (Advanced Features):\n",
    "   ✅ Temporal Windows: [7, 30, 90] giorni → 15 features\n",
    "   ✅ Lag Features: storico a 7d, 30d → 2 features\n",
    "   ✅ Ratio & Trend: confronti temporali → 6 features  \n",
    "   ✅ EWM Features: medie ponderate → 5 features\n",
    "   ✅ Calendar Features: stagionalità completa → 15 features\n",
    "\n",
    "2. 📈 TUO PROGETTO (Domain Knowledge):\n",
    "   ✅ Material encoding: target encoding per rm_id\n",
    "   ✅ Supplier analysis: diversity e frequency\n",
    "   ✅ Product features: statistical encoding\n",
    "   ✅ Missing value strategy: domain-specific → 9 features\n",
    "\n",
    "📊 OUTPUT FINALE:\n",
    "   Files: train_data.csv, test_data.csv\n",
    "   Features: 52 advanced features per ML\n",
    "   Split: Temporale (80/20) per time series\n",
    "   Target: net_weight (daily aggregated)\n",
    "   \n",
    "🚀 PROSSIMO STEP: baseline_model.ipynb\n",
    "   Input: train_data.csv, test_data.csv\n",
    "   Processo: CatBoost + Quantile Loss + Optuna\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntnu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
