{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c2aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f6a537",
   "metadata": {},
   "source": [
    "### Load the trained model and feature list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8141ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"🧪 TESTING BASELINE MODEL\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf074cb3",
   "metadata": {},
   "source": [
    "### Test 1: Load saved model and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a496f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🧪 TESTING BASELINE MODEL\n",
      "============================================================\n",
      "\n",
      "1️⃣ Test: Loading saved model and features\n",
      "   ✅ Model loaded successfully\n",
      "   ✅ Feature list loaded: 29 features\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1️⃣ Test: Loading saved model and features\")\n",
    "try:\n",
    "    with open('models/xgboost_baseline.pkl', 'rb') as f:\n",
    "        loaded_model = pickle.load(f)\n",
    "    \n",
    "    with open('models/feature_cols.pkl', 'rb') as f:\n",
    "        loaded_features = pickle.load(f)\n",
    "    \n",
    "    print(f\"   ✅ Model loaded successfully\")\n",
    "    print(f\"   ✅ Feature list loaded: {len(loaded_features)} features\")\n",
    "    assert isinstance(loaded_model, XGBRegressor), \"Model is not XGBRegressor\"\n",
    "    assert len(loaded_features) > 0, \"Feature list is empty\"\n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Failed to load model/features: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9384d967",
   "metadata": {},
   "source": [
    "### Test 2: Verify model attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e4ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2️⃣ Test: Model attributes\n",
      "   ✅ Model has required attributes\n",
      "   ✅ Model has 29 feature importances\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2️⃣ Test: Model attributes\")\n",
    "try:\n",
    "    assert hasattr(loaded_model, 'predict'), \"Model doesn't have predict method\"\n",
    "    assert hasattr(loaded_model, 'feature_importances_'), \"Model doesn't have feature importances\"\n",
    "    print(f\"   ✅ Model has required attributes\")\n",
    "    print(f\"   ✅ Model has {len(loaded_model.feature_importances_)} feature importances\")\n",
    "except AssertionError as e:\n",
    "    print(f\"   ❌ {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f28d3",
   "metadata": {},
   "source": [
    "### Test 3: Load and prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de2524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3️⃣ Test: Loading and preparing test data\n",
      "   ✅ Test data loaded: (122448, 10)\n",
      "   ✅ Date range: 2004-06-15 to 2024-12-19\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n3️⃣ Test: Loading and preparing test data\")\n",
    "try:\n",
    "    # Load test data (assuming it's preprocessed like train/test split)\n",
    "    test_data = pd.read_csv('../data/kernel/receivals.csv')\n",
    "    \n",
    "    # Convert date_arrival\n",
    "    test_data['date_arrival'] = pd.to_datetime(test_data['date_arrival'], format='%Y-%m-%d %H:%M:%S %z', utc=True, errors='coerce')\n",
    "    \n",
    "    # Filter only completed (assuming same preprocessing as training)\n",
    "    test_data = test_data[test_data['receival_status'] == 'Completed'].copy()\n",
    "    \n",
    "    print(f\"   ✅ Test data loaded: {test_data.shape}\")\n",
    "    print(f\"   ✅ Date range: {test_data['date_arrival'].min().date()} to {test_data['date_arrival'].max().date()}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Failed to load test data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4bc952",
   "metadata": {},
   "source": [
    "### Test 4: Feature engineering for test data (matching training pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1513ed80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4️⃣ Test: Feature engineering on test data\n",
      "   ✅ Features engineered: (122448, 36)\n",
      "   ✅ All required features present\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n4️⃣ Test: Feature engineering on test data\")\n",
    "try:\n",
    "    # Temporal features\n",
    "    test_data['year'] = test_data['date_arrival'].dt.year\n",
    "    test_data['month'] = test_data['date_arrival'].dt.month\n",
    "    test_data['day_of_week'] = test_data['date_arrival'].dt.dayofweek\n",
    "    test_data['quarter'] = test_data['date_arrival'].dt.quarter\n",
    "    test_data['is_weekend'] = test_data['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    test_data['month_sin'] = np.sin(2 * np.pi * test_data['month'] / 12)\n",
    "    test_data['month_cos'] = np.cos(2 * np.pi * test_data['month'] / 12)\n",
    "    test_data['day_sin'] = np.sin(2 * np.pi * test_data['day_of_week'] / 7)\n",
    "    test_data['day_cos'] = np.cos(2 * np.pi * test_data['day_of_week'] / 7)\n",
    "    \n",
    "    # Days since start\n",
    "    test_data['days_since_start'] = (test_data['date_arrival'] - test_data['date_arrival'].min()).dt.days\n",
    "    \n",
    "    # Supplier aggregations\n",
    "    supplier_stats = test_data.groupby('supplier_id')['net_weight'].agg([\n",
    "        ('mean', 'mean'),\n",
    "        ('median', 'median'),\n",
    "        ('std', 'std')\n",
    "    ]).reset_index()\n",
    "    supplier_stats.columns = ['supplier_id', 'supplier_mean_weight', 'supplier_median_weight', 'supplier_std_weight']\n",
    "    test_data = test_data.merge(supplier_stats, on='supplier_id', how='left')\n",
    "    \n",
    "    # Supplier CV and total receivals\n",
    "    test_data['supplier_cv'] = test_data['supplier_std_weight'] / test_data['supplier_mean_weight']\n",
    "    test_data['supplier_total_receivals'] = test_data.groupby('supplier_id')['supplier_id'].transform('count')\n",
    "    \n",
    "    # Lag features (simplified for test)\n",
    "    test_data = test_data.sort_values(['supplier_id', 'date_arrival'])\n",
    "    test_data['weight_lag_1'] = test_data.groupby('supplier_id')['net_weight'].shift(1)\n",
    "    test_data['weight_lag_3_mean'] = test_data.groupby('supplier_id')['net_weight'].transform(\n",
    "        lambda x: x.rolling(3, min_periods=1).mean().shift(1)\n",
    "    )\n",
    "    test_data['weight_lag_7_mean'] = test_data.groupby('supplier_id')['net_weight'].transform(\n",
    "        lambda x: x.rolling(7, min_periods=1).mean().shift(1)\n",
    "    )\n",
    "    \n",
    "    # Supplier trend\n",
    "    test_data['supplier_trend'] = test_data.groupby('supplier_id')['net_weight'].transform(\n",
    "        lambda x: x.rolling(10, min_periods=2).mean() / x.mean()\n",
    "    )\n",
    "    \n",
    "    # RM & Product features\n",
    "    rm_stats = test_data.groupby('rm_id')['net_weight'].agg(['mean', 'std', 'count']).reset_index()\n",
    "    rm_stats.columns = ['rm_id', 'rm_mean_weight', 'rm_std_weight', 'rm_count']\n",
    "    test_data = test_data.merge(rm_stats, on='rm_id', how='left')\n",
    "    \n",
    "    product_stats = test_data.groupby('product_id')['net_weight'].agg(['mean', 'std', 'count']).reset_index()\n",
    "    product_stats.columns = ['product_id', 'product_mean_weight', 'product_std_weight', 'product_count']\n",
    "    test_data = test_data.merge(product_stats, on='product_id', how='left')\n",
    "    \n",
    "    # Supplier-RM frequency\n",
    "    test_data['supplier_rm_frequency'] = test_data.groupby(['supplier_id', 'rm_id'])['rm_id'].transform('count')\n",
    "    \n",
    "    # Fill missing values\n",
    "    test_data = test_data.fillna(0)\n",
    "    \n",
    "    print(f\"   ✅ Features engineered: {test_data.shape}\")\n",
    "    \n",
    "    # Check if all required features exist\n",
    "    missing_features = [f for f in loaded_features if f not in test_data.columns]\n",
    "    if missing_features:\n",
    "        print(f\"   ⚠️  Missing features: {missing_features}\")\n",
    "    else:\n",
    "        print(f\"   ✅ All required features present\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Feature engineering failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a0a08",
   "metadata": {},
   "source": [
    "### Test 5: Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9a618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5️⃣ Test: Making predictions\n",
      "   ✅ Predictions generated: 122448 samples\n",
      "   ⚠️  9 negative predictions clipped to 0 (0.01%)\n",
      "   ⚠️  Most negative value: -1429.88 kg\n",
      "\n",
      "   📊 Prediction statistics (after clipping):\n",
      "      Mean: 12886.42 kg\n",
      "      Median: 12309.33 kg\n",
      "      Min: 0.00 kg\n",
      "      Max: 26259.20 kg\n",
      "      Std: 5760.13 kg\n",
      "      Q1 (25%): 8752.13 kg\n",
      "      Q3 (75%): 16995.49 kg\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5️⃣ Test: Making predictions\")\n",
    "try:\n",
    "    # Prepare feature matrix\n",
    "    X_test = test_data[loaded_features]\n",
    "    \n",
    "    # Make predictions\n",
    "    raw_predictions = loaded_model.predict(X_test)\n",
    "    \n",
    "    # Check for negative predictions\n",
    "    num_negative = (raw_predictions < 0).sum()\n",
    "    \n",
    "    # Clip negative predictions to 0\n",
    "    predictions = np.clip(raw_predictions, 0, None)\n",
    "    \n",
    "    print(f\"   ✅ Predictions generated: {len(predictions)} samples\")\n",
    "    \n",
    "    if num_negative > 0:\n",
    "        print(f\"   ⚠️  {num_negative} negative predictions clipped to 0 ({num_negative/len(predictions)*100:.2f}%)\")\n",
    "        print(f\"   ⚠️  Most negative value: {raw_predictions.min():.2f} kg\")\n",
    "    else:\n",
    "        print(f\"   ✅ All predictions are positive\")\n",
    "    \n",
    "    print(f\"\\n   📊 Prediction statistics (after clipping):\")\n",
    "    print(f\"      Mean: {predictions.mean():.2f} kg\")\n",
    "    print(f\"      Median: {np.median(predictions):.2f} kg\")\n",
    "    print(f\"      Min: {predictions.min():.2f} kg\")\n",
    "    print(f\"      Max: {predictions.max():.2f} kg\")\n",
    "    print(f\"      Std: {predictions.std():.2f} kg\")\n",
    "    print(f\"      Q1 (25%): {np.percentile(predictions, 25):.2f} kg\")\n",
    "    print(f\"      Q3 (75%): {np.percentile(predictions, 75):.2f} kg\")\n",
    "    \n",
    "    assert len(predictions) == len(X_test), \"Prediction length mismatch\"\n",
    "    assert not np.isnan(predictions).any(), \"Predictions contain NaN values\"\n",
    "    assert (predictions >= 0).all(), \"Predictions contain negative values after clipping\"\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Prediction failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e23c2",
   "metadata": {},
   "source": [
    "### Test 6: Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b6ca07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6️⃣ Test: Creating submission file\n",
      "   ✅ Submission file created: submission.csv\n",
      "   ✅ Shape: (122448, 2)\n",
      "\n",
      "   📄 First 10 rows:\n",
      " ID  predicted_weight\n",
      "  1      16997.349609\n",
      "  2      22451.980469\n",
      "  3      22261.490234\n",
      "  4      22298.880859\n",
      "  5      22298.880859\n",
      "  6      22489.380859\n",
      "  7      22549.189453\n",
      "  8      22606.460938\n",
      "  9      22606.460938\n",
      " 10      22643.849609\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n6️⃣ Test: Creating submission file\")\n",
    "try:\n",
    "    # Create submission dataframe\n",
    "    submission = pd.DataFrame({\n",
    "        'ID': range(1, len(predictions) + 1),\n",
    "        'predicted_weight': predictions\n",
    "    })\n",
    "    \n",
    "    # Round predictions to reasonable precision\n",
    "    submission['predicted_weight'] = submission['predicted_weight'].round(2)\n",
    "    \n",
    "    # Save submission file\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    print(f\"   ✅ Submission file created: submission.csv\")\n",
    "    print(f\"   ✅ Shape: {submission.shape}\")\n",
    "    print(f\"\\n   📄 First 10 rows:\")\n",
    "    print(submission.head(10).to_string(index=False))\n",
    "    \n",
    "    # Validate format\n",
    "    assert list(submission.columns) == ['ID', 'predicted_weight'], \"Column names don't match required format\"\n",
    "    assert submission['ID'].iloc[0] == 1, \"ID should start from 1\"\n",
    "    assert submission['ID'].is_monotonic_increasing, \"IDs should be sequential\"\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Submission file creation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f427d",
   "metadata": {},
   "source": [
    "### Test 7: Validate submission format against sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc74ae10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7️⃣ Test: Validating submission format\n",
      "   ℹ️  Sample submission shape: (30450, 2)\n",
      "   ℹ️  Generated submission shape: (122448, 2)\n",
      "   ✅ Submission format matches sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n7️⃣ Test: Validating submission format\")\n",
    "try:\n",
    "    sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "    \n",
    "    print(f\"   ℹ️  Sample submission shape: {sample_submission.shape}\")\n",
    "    print(f\"   ℹ️  Generated submission shape: {submission.shape}\")\n",
    "    \n",
    "    # Check column names match\n",
    "    assert list(submission.columns) == list(sample_submission.columns), \"Column names don't match sample\"\n",
    "    \n",
    "    print(f\"   ✅ Submission format matches sample_submission.csv\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️  Could not validate against sample: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18464c9b",
   "metadata": {},
   "source": [
    "### Test 8: Model performance check (if ground truth available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b34728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8️⃣ Test: Model performance check\n",
      "   📊 Performance on test data:\n",
      "      MAE: 4067.91 kg\n",
      "      R²: 0.5992\n",
      "      Relative Error: 31.35%\n",
      "   ❌ Model performance needs improvement (error >= 20%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n8️⃣ Test: Model performance check\")\n",
    "try:\n",
    "    if 'net_weight' in test_data.columns:\n",
    "        from sklearn.metrics import mean_absolute_error, r2_score\n",
    "        \n",
    "\n",
    "        y_true = test_data['net_weight'].values\n",
    "        mae = mean_absolute_error(y_true, predictions)\n",
    "        r2 = r2_score(y_true, predictions)\n",
    "        relative_error = (mae / y_true.mean()) * 100\n",
    "        \n",
    "        print(f\"   📊 Performance on test data:\")\n",
    "        print(f\"      MAE: {mae:.2f} kg\")\n",
    "        print(f\"      R²: {r2:.4f}\")\n",
    "        print(f\"      Relative Error: {relative_error:.2f}%\")\n",
    "        \n",
    "        # Performance threshold checks\n",
    "        if relative_error < 10:\n",
    "            print(f\"   ✅ Model performs well (error < 10%)\")\n",
    "        elif relative_error < 20:\n",
    "            print(f\"   ⚠️  Model performance acceptable (error < 20%)\")\n",
    "        else:\n",
    "            print(f\"   ❌ Model performance needs improvement (error >= 20%)\")\n",
    "    else:\n",
    "        print(f\"   ℹ️  Ground truth not available, skipping performance check\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️  Performance check failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f57b108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "✅ ALL TESTS COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "📁 Output file: submission.csv\n",
      "📊 Predictions: 122448 rows\n",
      "🎯 Ready for submission!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✅ ALL TESTS COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n📁 Output file: submission.csv\")\n",
    "print(f\"📊 Predictions: {len(predictions)} rows\")\n",
    "print(f\"🎯 Ready for submission!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntnu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
