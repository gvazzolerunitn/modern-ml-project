{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216681e3",
   "metadata": {},
   "source": [
    "# Supply Chain Forecasting: Advanced Gradient Boosting Approach\n",
    "\n",
    "**Objective:** Predict cumulative raw material deliveries for January-May 2025\n",
    "\n",
    "**Strategy:**\n",
    "- Window-based feature engineering (not sequence-based)\n",
    "- Rich historical aggregates from multiple data sources\n",
    "- LightGBM with quantile regression (Œ±=0.2)\n",
    "- Time-aware validation and monotonicity enforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10f765a",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8371d7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment configured\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Tuple\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"‚úÖ Environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3072edfb",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b568b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading datasets...\n",
      "Receivals: (122590, 10)\n",
      "Purchase Orders: (33171, 12)\n",
      "Materials: (1218, 6)\n",
      "Prediction Mapping: (30450, 4)\n",
      "‚úÖ Data loaded and cleaned\n"
     ]
    }
   ],
   "source": [
    "print(\"üìÇ Loading datasets...\")\n",
    "\n",
    "# Load core datasets\n",
    "receivals = pd.read_csv('data/kernel/receivals.csv', parse_dates=['date_arrival'])\n",
    "purchase_orders = pd.read_csv('data/kernel/purchase_orders.csv', parse_dates=['delivery_date'])\n",
    "materials = pd.read_csv('data/extended/materials.csv')\n",
    "prediction_mapping = pd.read_csv('data/prediction_mapping.csv')\n",
    "\n",
    "# Parse dates in mapping\n",
    "prediction_mapping['forecast_start_date'] = pd.to_datetime(prediction_mapping['forecast_start_date'], utc=True)\n",
    "prediction_mapping['forecast_end_date'] = pd.to_datetime(prediction_mapping['forecast_end_date'], utc=True)\n",
    "\n",
    "print(f\"Receivals: {receivals.shape}\")\n",
    "print(f\"Purchase Orders: {purchase_orders.shape}\")\n",
    "print(f\"Materials: {materials.shape}\")\n",
    "print(f\"Prediction Mapping: {prediction_mapping.shape}\")\n",
    "\n",
    "# Data quality: remove invalid entries\n",
    "receivals = receivals[receivals['net_weight'] > 0].copy()\n",
    "purchase_orders = purchase_orders[purchase_orders['quantity'] > 0].copy()\n",
    "\n",
    "# Convert pounds to kg (unit_id 43 = pounds)\n",
    "LBS_TO_KG = 0.453592\n",
    "mask_lbs = purchase_orders['unit_id'] == 43\n",
    "purchase_orders.loc[mask_lbs, 'quantity'] *= LBS_TO_KG\n",
    "purchase_orders.loc[mask_lbs, 'unit_id'] = 40  # kg unit_id\n",
    "\n",
    "print(\"‚úÖ Data loaded and cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c837fd",
   "metadata": {},
   "source": [
    "## 3. Create Product-Material Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d31af4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Linked 340195 purchase orders to raw materials\n"
     ]
    }
   ],
   "source": [
    "# Build rm_id to product_id mapping from materials\n",
    "product_to_rm = materials[['product_id', 'rm_id']].drop_duplicates()\n",
    "product_to_rm = product_to_rm.dropna(subset=['product_id', 'rm_id'])\n",
    "\n",
    "# Link purchase orders to raw materials\n",
    "po_with_rm = purchase_orders.merge(\n",
    "    product_to_rm,\n",
    "    on='product_id',\n",
    "    how='left'\n",
    ")\n",
    "po_with_rm = po_with_rm.dropna(subset=['rm_id'])\n",
    "\n",
    "print(f\"‚úÖ Linked {len(po_with_rm)} purchase orders to raw materials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afaa89b",
   "metadata": {},
   "source": [
    "## 4. Daily Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0f141e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating daily aggregations...\n",
      "‚úÖ Daily receivals: (41906, 5)\n",
      "   Date range: 2004-06-15 00:00:00 to 2024-12-19 00:00:00\n",
      "‚úÖ Daily POs: (43996, 4)\n",
      "   Date range: 2002-04-29 00:00:00 to 2025-06-29 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Aggregate receivals and POs to daily level per material\n",
    "\n",
    "print(\"üîÑ Creating daily aggregations...\")\n",
    "\n",
    "# Ensure datetime (without timezone issues)\n",
    "receivals['date_arrival'] = pd.to_datetime(receivals['date_arrival'], utc=True).dt.tz_localize(None)\n",
    "po_with_rm['delivery_date'] = pd.to_datetime(po_with_rm['delivery_date'], utc=True).dt.tz_localize(None)\n",
    "\n",
    "# Daily receivals per material (use normalize() instead of dt.date)\n",
    "daily_receivals = receivals.copy()\n",
    "daily_receivals['date'] = daily_receivals['date_arrival'].dt.normalize()\n",
    "\n",
    "daily_receivals = daily_receivals.groupby(['date', 'rm_id']).agg({\n",
    "    'net_weight': ['sum', 'count', 'mean']\n",
    "}).reset_index()\n",
    "\n",
    "daily_receivals.columns = ['date', 'rm_id', 'weight_sum', 'delivery_count', 'weight_mean']\n",
    "\n",
    "# Daily purchase orders per material\n",
    "daily_pos = po_with_rm.copy()\n",
    "daily_pos['date'] = daily_pos['delivery_date'].dt.normalize()\n",
    "\n",
    "daily_pos = daily_pos.groupby(['date', 'rm_id']).agg({\n",
    "    'quantity': ['sum', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "daily_pos.columns = ['date', 'rm_id', 'po_quantity', 'po_count']\n",
    "\n",
    "print(f\"‚úÖ Daily receivals: {daily_receivals.shape}\")\n",
    "print(f\"   Date range: {daily_receivals['date'].min()} to {daily_receivals['date'].max()}\")\n",
    "print(f\"‚úÖ Daily POs: {daily_pos.shape}\")\n",
    "print(f\"   Date range: {daily_pos['date'].min()} to {daily_pos['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6cc752",
   "metadata": {},
   "source": [
    "## 5. Advanced Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd7271e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature engineering functions defined\n"
     ]
    }
   ],
   "source": [
    "def calculate_window_receivals(df: pd.DataFrame, start_date: datetime, \n",
    "                                end_date: datetime, rm_id: int) -> float:\n",
    "    \"\"\"Calculate total receivals in a window using vectorized operations\"\"\"\n",
    "    mask = (\n",
    "        (df['rm_id'] == rm_id) &\n",
    "        (df['date'] >= start_date) &\n",
    "        (df['date'] <= end_date)\n",
    "    )\n",
    "    return df.loc[mask, 'weight_sum'].sum()\n",
    "\n",
    "def calculate_historical_stats(daily_df: pd.DataFrame, ref_date: datetime, \n",
    "                                rm_id: int, lookback_days: int) -> Dict:\n",
    "    \"\"\"Calculate historical statistics for a material\"\"\"\n",
    "    start = ref_date - timedelta(days=lookback_days)\n",
    "    mask = (\n",
    "        (daily_df['rm_id'] == rm_id) &\n",
    "        (daily_df['date'] >= start) &\n",
    "        (daily_df['date'] < ref_date)\n",
    "    )\n",
    "    \n",
    "    subset = daily_df.loc[mask]\n",
    "    \n",
    "    if len(subset) == 0:\n",
    "        return {\n",
    "            'sum': 0.0,\n",
    "            'mean': 0.0,\n",
    "            'std': 0.0,\n",
    "            'count': 0\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'sum': subset['weight_sum'].sum(),\n",
    "        'mean': subset['weight_sum'].mean(),\n",
    "        'std': subset['weight_sum'].std() if len(subset) > 1 else 0.0,\n",
    "        'count': len(subset)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Feature engineering functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e0ab03",
   "metadata": {},
   "source": [
    "## 5.5 Debug: Check Data Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e376e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debugging data availability...\n",
      "\n",
      "Daily receivals shape: (41906, 5)\n",
      "Daily receivals date range: 2004-06-15 00:00:00 to 2024-12-19 00:00:00\n",
      "Unique materials in receivals: 203\n",
      "\n",
      "Daily POs shape: (43996, 4)\n",
      "Daily POs date range: 2002-04-29 00:00:00 to 2025-06-29 00:00:00\n",
      "Unique materials in POs: 202\n",
      "\n",
      "Receival rows before 2025: 41906\n",
      "\n",
      "Top 10 materials by receival frequency:\n",
      "rm_id\n",
      "2130.0    2940\n",
      "2160.0    2450\n",
      "2142.0    2312\n",
      "2140.0    2172\n",
      "2134.0    1787\n",
      "2132.0    1735\n",
      "2135.0    1562\n",
      "2131.0    1518\n",
      "2144.0    1515\n",
      "1903.0    1512\n",
      "dtype: int64\n",
      "\n",
      "Test material 2130.0:\n",
      "  Receivals: 2940\n",
      "  Date range: 2011-11-15 00:00:00 to 2024-12-19 00:00:00\n",
      "  Total weight: 351244347.0\n",
      "\n",
      "Test window calculation for 2130.0 (2020-01-01 to 2020-01-30): 2766190.0\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Debugging data availability...\")\n",
    "print(f\"\\nDaily receivals shape: {daily_receivals.shape}\")\n",
    "print(f\"Daily receivals date range: {daily_receivals['date'].min()} to {daily_receivals['date'].max()}\")\n",
    "print(f\"Unique materials in receivals: {daily_receivals['rm_id'].nunique()}\")\n",
    "print(f\"\\nDaily POs shape: {daily_pos.shape}\")\n",
    "print(f\"Daily POs date range: {daily_pos['date'].min()} to {daily_pos['date'].max()}\")\n",
    "print(f\"Unique materials in POs: {daily_pos['rm_id'].nunique()}\")\n",
    "\n",
    "# Check if dates are before 2025\n",
    "receivals_before_2025 = daily_receivals[daily_receivals['date'] < pd.to_datetime('2025-01-01')]\n",
    "print(f\"\\nReceival rows before 2025: {len(receivals_before_2025)}\")\n",
    "\n",
    "# Top 10 materials\n",
    "top_10_materials = daily_receivals.groupby('rm_id').size().sort_values(ascending=False).head(10)\n",
    "print(f\"\\nTop 10 materials by receival frequency:\")\n",
    "print(top_10_materials)\n",
    "\n",
    "# Sample a specific material to test\n",
    "test_rm = top_10_materials.index[0]\n",
    "test_data = daily_receivals[daily_receivals['rm_id'] == test_rm]\n",
    "print(f\"\\nTest material {test_rm}:\")\n",
    "print(f\"  Receivals: {len(test_data)}\")\n",
    "print(f\"  Date range: {test_data['date'].min()} to {test_data['date'].max()}\")\n",
    "print(f\"  Total weight: {test_data['weight_sum'].sum()}\")\n",
    "\n",
    "# Test calculate_window_receivals function\n",
    "test_start = datetime(2020, 1, 1)\n",
    "test_end = datetime(2020, 1, 30)\n",
    "test_target = calculate_window_receivals(daily_receivals, test_start, test_end, test_rm)\n",
    "print(f\"\\nTest window calculation for {test_rm} (2020-01-01 to 2020-01-30): {test_target}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e2b820",
   "metadata": {},
   "source": [
    "## 6-8. Optimized Training Data Generation with Features and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d901ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating training dataset with temporal weighting...\n",
      "Temporal weights:\n",
      "  2016: 0.3x\n",
      "  2017: 0.4x\n",
      "  2018: 0.6x\n",
      "  2019: 0.8x\n",
      "  2020: 1.0x\n",
      "  2021: 1.3x\n",
      "  2022: 1.6x\n",
      "  2023: 1.9x\n",
      "  2024: 2.5x\n",
      "\n",
      "Using 120 materials from years [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
      "  Processing year 2016 (weight=0.3x)...\n",
      "    Year 2016: 260 samples\n",
      "  Processing year 2017 (weight=0.4x)...\n",
      "    Year 2017: 266 samples\n",
      "  Processing year 2018 (weight=0.6x)...\n",
      "    Year 2018: 239 samples\n",
      "  Processing year 2019 (weight=0.8x)...\n",
      "    Year 2019: 281 samples\n",
      "  Processing year 2020 (weight=1.0x)...\n",
      "    Year 2020: 238 samples\n",
      "  Processing year 2021 (weight=1.3x)...\n",
      "    Year 2021: 306 samples\n",
      "  Processing year 2022 (weight=1.6x)...\n",
      "    Year 2022: 330 samples\n",
      "  Processing year 2023 (weight=1.9x)...\n",
      "    Year 2023: 329 samples\n",
      "  Processing year 2024 (weight=2.5x)...\n",
      "    Year 2024: 360 samples\n",
      "\n",
      "‚úÖ Training dataset with temporal weighting: (2609, 14)\n",
      "Sample weight distribution:\n",
      "  Min weight: 0.241\n",
      "  Max weight: 2.012\n",
      "  Mean weight: 1.000\n",
      "\n",
      "Breakdown by year:\n",
      "  2016:    260 samples ( 10.0%) | Total weight:     83.7\n",
      "  2017:    266 samples ( 10.2%) | Total weight:     86.6\n",
      "  2018:    239 samples (  9.2%) | Total weight:     75.3\n",
      "  2019:    281 samples ( 10.8%) | Total weight:     96.3\n",
      "  2020:    238 samples (  9.1%) | Total weight:     74.9\n",
      "  2021:    306 samples ( 11.7%) | Total weight:    112.4\n",
      "  2022:    330 samples ( 12.6%) | Total weight:    127.8\n",
      "  2023:    329 samples ( 12.6%) | Total weight:    127.2\n",
      "  2024:    360 samples ( 13.8%) | Total weight:    147.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give MORE weight to recent data, LESS weight to old data\n",
    "\n",
    "print(\"üîÑ Generating training dataset with temporal weighting...\")\n",
    "\n",
    "# Use full history (2016-2024) ma con pesi decrescenti verso il passato\n",
    "TRAINING_YEARS = [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
    "WINDOW_STEPS = [1, 3, 5, 7, 10, 14, 21, 30, 45, 60, 75, 90, 105, 120, 151]\n",
    "\n",
    "# Define temporal weights (exponential increase towards 2024)\n",
    "year_weights = {\n",
    "    2016: 0.3,\n",
    "    2017: 0.4,\n",
    "    2018: 0.6,\n",
    "    2019: 0.8,\n",
    "    2020: 1.0,\n",
    "    2021: 1.3,\n",
    "    2022: 1.6,\n",
    "    2023: 1.9,\n",
    "    2024: 2.5   # 2024 √® 8x pi√π importante di 2016\n",
    "}\n",
    "\n",
    "print(\"Temporal weights:\")\n",
    "for year in TRAINING_YEARS:\n",
    "    print(f\"  {year}: {year_weights[year]:.1f}x\")\n",
    "\n",
    "# Get top materials\n",
    "top_materials = (\n",
    "    daily_receivals.groupby('rm_id')\n",
    "    .size()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(120)\n",
    "    .index.tolist()\n",
    ")\n",
    "\n",
    "print(f\"\\nUsing {len(top_materials)} materials from years {TRAINING_YEARS}\")\n",
    "\n",
    "training_data_list = []\n",
    "sample_weights_list = []\n",
    "year_totals = {}\n",
    "\n",
    "for year in TRAINING_YEARS:\n",
    "    print(f\"  Processing year {year} (weight={year_weights[year]:.1f}x)...\")\n",
    "    year_start = datetime(year, 1, 1)\n",
    "    year_total = 0\n",
    "    year_weight = year_weights[year]\n",
    "    \n",
    "    for rm_id in top_materials:\n",
    "        for window_days in WINDOW_STEPS:\n",
    "            start_date = year_start\n",
    "            end_date = year_start + timedelta(days=window_days - 1)\n",
    "            \n",
    "            if end_date >= datetime(2025, 1, 1):\n",
    "                continue\n",
    "            \n",
    "            target = calculate_window_receivals(\n",
    "                daily_receivals, start_date, end_date, rm_id\n",
    "            )\n",
    "            \n",
    "            if target <= 0:\n",
    "                continue\n",
    "            \n",
    "            row = {\n",
    "                'rm_id': rm_id,\n",
    "                'start_date': start_date,\n",
    "                'end_date': end_date,\n",
    "                'window_days': window_days,\n",
    "                'end_month': end_date.month,\n",
    "                'end_quarter': (end_date.month - 1) // 3 + 1,\n",
    "                'end_dayofyear': end_date.timetuple().tm_yday,\n",
    "                'is_q1': 1 if end_date.month <= 3 else 0,\n",
    "                'target': target\n",
    "            }\n",
    "            \n",
    "            # Historical features\n",
    "            ref_date = start_date\n",
    "            rec_stats = calculate_historical_stats(daily_receivals, ref_date, rm_id, 30)\n",
    "            row['rec_hist_30d_sum'] = rec_stats['sum']\n",
    "            row['rec_hist_30d_mean'] = rec_stats['mean']\n",
    "            \n",
    "            po_hist_mask = (\n",
    "                (daily_pos['rm_id'] == rm_id) &\n",
    "                (daily_pos['date'] >= ref_date - timedelta(days=30)) &\n",
    "                (daily_pos['date'] < ref_date)\n",
    "            )\n",
    "            row['po_hist_30d_sum'] = daily_pos.loc[po_hist_mask, 'po_quantity'].sum()\n",
    "            \n",
    "            po_window_mask = (\n",
    "                (daily_pos['rm_id'] == rm_id) &\n",
    "                (daily_pos['date'] >= start_date) &\n",
    "                (daily_pos['date'] <= end_date)\n",
    "            )\n",
    "            row['po_in_window'] = daily_pos.loc[po_window_mask, 'po_quantity'].sum()\n",
    "            \n",
    "            start_ly = start_date - timedelta(days=365)\n",
    "            end_ly = end_date - timedelta(days=365)\n",
    "            rec_ly_mask = (\n",
    "                (daily_receivals['rm_id'] == rm_id) &\n",
    "                (daily_receivals['date'] >= start_ly) &\n",
    "                (daily_receivals['date'] <= end_ly)\n",
    "            )\n",
    "            row['same_period_last_year'] = daily_receivals.loc[rec_ly_mask, 'weight_sum'].sum()\n",
    "            \n",
    "            training_data_list.append(row)\n",
    "            sample_weights_list.append(year_weight)  # ‚Üê WEIGHT per sample\n",
    "            year_total += 1\n",
    "    \n",
    "    year_totals[year] = year_total\n",
    "    print(f\"    Year {year}: {year_total} samples\")\n",
    "    gc.collect()\n",
    "\n",
    "train_df = pd.DataFrame(training_data_list)\n",
    "sample_weights = np.array(sample_weights_list)\n",
    "\n",
    "# Normalize weights to sum to 1 (oppure no, a seconda della libreria)\n",
    "sample_weights = sample_weights / sample_weights.mean()  # Normalization\n",
    "\n",
    "print(f\"\\n‚úÖ Training dataset with temporal weighting: {train_df.shape}\")\n",
    "print(f\"Sample weight distribution:\")\n",
    "print(f\"  Min weight: {sample_weights.min():.3f}\")\n",
    "print(f\"  Max weight: {sample_weights.max():.3f}\")\n",
    "print(f\"  Mean weight: {sample_weights.mean():.3f}\")\n",
    "\n",
    "print(f\"\\nBreakdown by year:\")\n",
    "for year in TRAINING_YEARS:\n",
    "    count = year_totals.get(year, 0)\n",
    "    pct = (count / len(train_df)) * 100 if len(train_df) > 0 else 0\n",
    "    total_weight = sample_weights[year_totals[year]:year_totals[year] + count].sum() if count > 0 else 0\n",
    "    print(f\"  {year}: {count:6d} samples ({pct:5.1f}%) | Total weight: {total_weight:8.1f}\")\n",
    "\n",
    "del training_data_list, sample_weights_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23c1fc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Adding advanced features...\n",
      "‚úÖ Advanced features added\n",
      "   Features: Index(['volatility_30d', 'ma_7d', 'ma_30d', 'lag_60d'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 6.5 Advanced Feature Engineering\n",
    "\n",
    "# %%\n",
    "print(\"üîß Adding advanced features...\")\n",
    "\n",
    "# Per ogni campione, calcola features avanzate\n",
    "def add_advanced_features(train_df, daily_receivals, daily_pos):\n",
    "    \"\"\"Add volatility, MA, lag features\"\"\"\n",
    "    \n",
    "    train_df = train_df.copy()\n",
    "    \n",
    "    advanced_features = []\n",
    "    \n",
    "    for idx, row in train_df.iterrows():\n",
    "        rm_id = row['rm_id']\n",
    "        ref_date = row['start_date']\n",
    "        window_days = row['window_days']\n",
    "        \n",
    "        # 1. VOLATILITY (std dev dei 30 giorni precedenti)\n",
    "        rec_hist = daily_receivals[\n",
    "            (daily_receivals['rm_id'] == rm_id) &\n",
    "            (daily_receivals['date'] >= ref_date - timedelta(days=30)) &\n",
    "            (daily_receivals['date'] < ref_date)\n",
    "        ]\n",
    "        volatility = rec_hist['weight_sum'].std() if len(rec_hist) > 1 else 0\n",
    "        \n",
    "        # 2. MOVING AVERAGES\n",
    "        ma_7 = rec_hist['weight_sum'].tail(7).mean() if len(rec_hist) >= 7 else 0\n",
    "        ma_30 = rec_hist['weight_sum'].mean()\n",
    "        \n",
    "        # 3. LAG FEATURES (deliveries 60 giorni fa)\n",
    "        lag_60_start = ref_date - timedelta(days=120)\n",
    "        lag_60_end = ref_date - timedelta(days=60)\n",
    "        lag_60_data = daily_receivals[\n",
    "            (daily_receivals['rm_id'] == rm_id) &\n",
    "            (daily_receivals['date'] >= lag_60_start) &\n",
    "            (daily_receivals['date'] <= lag_60_end)\n",
    "        ]\n",
    "        lag_60 = lag_60_data['weight_sum'].sum()\n",
    "        \n",
    "        advanced_features.append({\n",
    "            'volatility_30d': volatility,\n",
    "            'ma_7d': ma_7,\n",
    "            'ma_30d': ma_30,\n",
    "            'lag_60d': lag_60\n",
    "        })\n",
    "    \n",
    "    adv_df = pd.DataFrame(advanced_features)\n",
    "    for col in adv_df.columns:\n",
    "        train_df[col] = adv_df[col]\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "train_df = add_advanced_features(train_df, daily_receivals, daily_pos)\n",
    "print(f\"‚úÖ Advanced features added\")\n",
    "print(f\"   Features: {train_df.columns[-4:]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be486fc2",
   "metadata": {},
   "source": [
    "## 9. Prepare Training and Validation Sets (WITH LOG TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "493cac28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Creating PROPER time-series split...\n",
      "‚úÖ Time-Series Split (ORIGINAL SCALE):\n",
      "   Training: 2249 samples\n",
      "   Validation: 360 samples\n",
      "   y_train range: 143 to 14742968 kg\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 9. Prepare Training and Validation Sets\n",
    "\n",
    "# %%\n",
    "print(\"üìä Creating PROPER time-series split...\")\n",
    "\n",
    "feature_cols = [\n",
    "    'rm_id', 'window_days', 'end_month', 'end_quarter', \n",
    "    'end_dayofyear', 'is_q1',\n",
    "    'rec_hist_30d_sum', 'rec_hist_30d_mean',\n",
    "    'po_hist_30d_sum', 'po_in_window', \n",
    "    'same_period_last_year',\n",
    "    'volatility_30d', 'ma_7d', 'ma_30d', 'lag_60d'\n",
    "]\n",
    "\n",
    "# Split RIGIDO per anno\n",
    "train_mask = train_df['end_date'].dt.year < 2024\n",
    "val_mask = train_df['end_date'].dt.year == 2024\n",
    "\n",
    "X_train = train_df[train_mask].copy()\n",
    "X_val = train_df[val_mask].copy()\n",
    "\n",
    "# ‚Üê USA TARGET ORIGINALE (NOT LOG)\n",
    "y_train = X_train['target']  # ‚Üê ORIGINAL SCALE\n",
    "y_val = X_val['target']      # ‚Üê ORIGINAL SCALE\n",
    "\n",
    "# Sample weights\n",
    "train_indices = X_train.index.tolist()\n",
    "val_indices = X_val.index.tolist()\n",
    "\n",
    "sample_weights_train = sample_weights[train_indices]\n",
    "sample_weights_val = sample_weights[val_indices]\n",
    "\n",
    "X_train_features = X_train[feature_cols].copy()\n",
    "X_val_features = X_val[feature_cols].copy()\n",
    "\n",
    "X_train_features['rm_id'] = X_train_features['rm_id'].astype(float)\n",
    "X_val_features['rm_id'] = X_val_features['rm_id'].astype(float)\n",
    "\n",
    "print(f\"‚úÖ Time-Series Split (ORIGINAL SCALE):\")\n",
    "print(f\"   Training: {X_train_features.shape[0]} samples\")\n",
    "print(f\"   Validation: {X_val_features.shape[0]} samples\")\n",
    "print(f\"   y_train range: {y_train.min():.0f} to {y_train.max():.0f} kg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9de44",
   "metadata": {},
   "source": [
    "## 10. Train LightGBM Model with Temporal Sample Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2998e037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training LightGBM with STRONGER regularization...\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[100]\tvalid_0's quantile: 85275\n",
      "[200]\tvalid_0's quantile: 76820.8\n",
      "[300]\tvalid_0's quantile: 69377.1\n",
      "[400]\tvalid_0's quantile: 64716.6\n",
      "[500]\tvalid_0's quantile: 67685\n",
      "Early stopping, best iteration is:\n",
      "[424]\tvalid_0's quantile: 64507.6\n",
      "‚úÖ Training completed with STRONG regularization\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 10. Train LightGBM (STRONGER Regularization)\n",
    "\n",
    "# %%\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"üöÄ Training LightGBM with STRONGER regularization...\")\n",
    "\n",
    "model = LGBMRegressor(\n",
    "    objective='quantile',\n",
    "    alpha=0.2,\n",
    "    n_estimators=1500,  # Ridotto da 2000\n",
    "    learning_rate=0.01,  # Ridotto da 0.03 ‚Üê CHIAVE\n",
    "    max_depth=5,  # Ridotto da 8 ‚Üê CHIAVE\n",
    "    num_leaves=32,  # Ridotto da 64 ‚Üê CHIAVE\n",
    "    min_child_samples=50,  # Aumentato da 20 ‚Üê CHIAVE\n",
    "    subsample=0.7,  # Ridotto da 0.8\n",
    "    colsample_bytree=0.7,  # Ridotto da 0.8\n",
    "    reg_alpha=1.0,  # Aumentato da 0.1 ‚Üê CHIAVE\n",
    "    reg_lambda=2.0,  # Aumentato da 1.0 ‚Üê CHIAVE\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train_features, y_train,\n",
    "    sample_weight=sample_weights_train,\n",
    "    eval_set=[(X_val_features, y_val)],\n",
    "    eval_sample_weight=[sample_weights_val],\n",
    "    eval_metric='quantile',\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=150, verbose=True),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Training completed with STRONG regularization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e0b283",
   "metadata": {},
   "source": [
    "## 10.5 Train CatBoost for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b24339f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training CatBoost for comparison...\n",
      "0:\tlearn: 117015.1353223\ttest: 92071.6135579\tbest: 92071.6135579 (0)\ttotal: 48ms\tremaining: 1m 12s\n",
      "100:\tlearn: 101010.9813686\ttest: 81952.2731676\tbest: 81952.2731676 (100)\ttotal: 118ms\tremaining: 1.64s\n",
      "200:\tlearn: 90667.7266279\ttest: 75620.3542624\tbest: 75620.3542624 (200)\ttotal: 193ms\tremaining: 1.25s\n",
      "300:\tlearn: 78274.0430699\ttest: 69202.3355396\tbest: 69202.3355396 (300)\ttotal: 273ms\tremaining: 1.09s\n",
      "400:\tlearn: 63366.3479273\ttest: 70935.4861958\tbest: 68248.5433871 (331)\ttotal: 352ms\tremaining: 964ms\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 68248.54339\n",
      "bestIteration = 331\n",
      "\n",
      "Shrink model to first 332 iterations.\n",
      "\n",
      "üìä Model Comparison (Validation):\n",
      "==================================================\n",
      "LightGBM MAE:      288,526 kg\n",
      "CatBoost MAE:      307,022 kg\n",
      "Difference:         18,496 kg\n",
      "‚úÖ LightGBM is 6.4% BETTER\n",
      "   ‚Üí Keep LightGBM\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from catboost import CatBoostRegressor\n",
    "import catboost as cat\n",
    "\n",
    "print(\"üöÄ Training CatBoost for comparison...\")\n",
    "\n",
    "catboost_model = CatBoostRegressor(\n",
    "    loss_function='Quantile:alpha=0.2',  # Quantile 0.2\n",
    "    iterations=1500,\n",
    "    learning_rate=0.01,\n",
    "    depth=5,\n",
    "    l2_leaf_reg=2.0,\n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    "    thread_count=-1\n",
    ")\n",
    "\n",
    "catboost_model.fit(\n",
    "    X_train_features, y_train,\n",
    "    sample_weight=sample_weights_train,\n",
    "    eval_set=(X_val_features, y_val),\n",
    "    early_stopping_rounds=150,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Predict on validation\n",
    "cat_pred_val = catboost_model.predict(X_val_features)\n",
    "cat_mae = np.mean(np.abs(cat_pred_val - y_val))\n",
    "\n",
    "# Compare with LightGBM\n",
    "lgb_pred_val = model.predict(X_val_features)\n",
    "lgb_mae = np.mean(np.abs(lgb_pred_val - y_val))\n",
    "\n",
    "print(f\"\\nüìä Model Comparison (Validation):\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"LightGBM MAE: {lgb_mae:>12,.0f} kg\")\n",
    "print(f\"CatBoost MAE: {cat_mae:>12,.0f} kg\")\n",
    "print(f\"Difference:   {abs(cat_mae - lgb_mae):>12,.0f} kg\")\n",
    "\n",
    "if cat_mae < lgb_mae:\n",
    "    print(f\"‚úÖ CatBoost is {((lgb_mae - cat_mae) / lgb_mae * 100):.1f}% BETTER\")\n",
    "    print(f\"   ‚Üí Use CatBoost for final submission\")\n",
    "    use_catboost = True\n",
    "else:\n",
    "    print(f\"‚úÖ LightGBM is {((cat_mae - lgb_mae) / lgb_mae * 100):.1f}% BETTER\")\n",
    "    print(f\"   ‚Üí Keep LightGBM\")\n",
    "    use_catboost = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb33c81",
   "metadata": {},
   "source": [
    "## 11. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2547c16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Top 15 Most Important Features:\n",
      "              feature  importance\n",
      "         po_in_window        1134\n",
      "              lag_60d         902\n",
      "same_period_last_year         649\n",
      "                rm_id         643\n",
      "          window_days         572\n",
      "     rec_hist_30d_sum         571\n",
      "    rec_hist_30d_mean         485\n",
      "               ma_30d         353\n",
      "      po_hist_30d_sum         331\n",
      "                ma_7d         245\n",
      "       volatility_30d         173\n",
      "        end_dayofyear         139\n",
      "            end_month         102\n",
      "          end_quarter           6\n",
      "                is_q1           1\n"
     ]
    }
   ],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìà Top 15 Most Important Features:\")\n",
    "print(feature_importance.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d093f994",
   "metadata": {},
   "source": [
    "## 12. Refit on Full Dataset (WITH LOG TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a67e83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Refitting model on full training data...\n",
      "‚úÖ Final model trained on full dataset (LOG SCALE)\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ Refitting model on full training data...\")\n",
    "\n",
    "# Combine train + validation\n",
    "X_full = pd.concat([X_train_features, X_val_features])\n",
    "y_full = pd.concat([y_train, y_val])  # ‚Üê Already in LOG space\n",
    "\n",
    "# Combine weights\n",
    "sample_weights_full = np.concatenate([sample_weights_train, sample_weights_val])\n",
    "\n",
    "# Use best_iteration from previous training\n",
    "final_model = LGBMRegressor(\n",
    "    objective='quantile',\n",
    "    alpha=0.2,\n",
    "    n_estimators=model.best_iteration_ if model.best_iteration_ > 0 else 500,\n",
    "    learning_rate=0.01,  # ‚Üê Keep strong regularization\n",
    "    num_leaves=32,\n",
    "    max_depth=5,\n",
    "    min_child_samples=50,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=2.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "final_model.fit(X_full, y_full, sample_weight=sample_weights_full)\n",
    "\n",
    "print(\"‚úÖ Final model trained on full dataset (LOG SCALE)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482c756b",
   "metadata": {},
   "source": [
    "## 13. Prepare Test Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ebe7a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating test features...\n",
      "  Computing historical features (30d)...\n",
      "  Computing POs in test windows...\n",
      "  Computing year-over-year...\n",
      "  Computing advanced features...\n",
      "‚úÖ Test features ready: (30450, 15)\n",
      "   Features: 15 (same as training)\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß Creating test features...\")\n",
    "\n",
    "test_df = prediction_mapping.copy()\n",
    "\n",
    "# Remove timezone\n",
    "test_df['forecast_start_date'] = pd.to_datetime(test_df['forecast_start_date']).dt.tz_localize(None)\n",
    "test_df['forecast_end_date'] = pd.to_datetime(test_df['forecast_end_date']).dt.tz_localize(None)\n",
    "\n",
    "# Basic date features\n",
    "test_df['window_days'] = (test_df['forecast_end_date'] - test_df['forecast_start_date']).dt.days + 1\n",
    "test_df['end_month'] = test_df['forecast_end_date'].dt.month\n",
    "test_df['end_quarter'] = test_df['forecast_end_date'].dt.quarter\n",
    "test_df['end_dayofyear'] = test_df['forecast_end_date'].dt.dayofyear\n",
    "test_df['is_q1'] = (test_df['end_quarter'] == 1).astype(int)\n",
    "\n",
    "# Historical features (30d)\n",
    "print(\"  Computing historical features (30d)...\")\n",
    "\n",
    "hist_features_30d = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    ref_date = row['forecast_start_date']\n",
    "    rm_id = row['rm_id']\n",
    "    \n",
    "    rec_stats = calculate_historical_stats(daily_receivals, ref_date, rm_id, 30)\n",
    "    \n",
    "    po_mask = (\n",
    "        (daily_pos['rm_id'] == rm_id) &\n",
    "        (daily_pos['date'] >= ref_date - timedelta(days=30)) &\n",
    "        (daily_pos['date'] < ref_date)\n",
    "    )\n",
    "    po_sum = daily_pos.loc[po_mask, 'po_quantity'].sum()\n",
    "    \n",
    "    hist_features_30d.append({\n",
    "        'rec_hist_30d_sum': rec_stats['sum'],\n",
    "        'rec_hist_30d_mean': rec_stats['mean'],\n",
    "        'po_hist_30d_sum': po_sum\n",
    "    })\n",
    "\n",
    "hist_df = pd.DataFrame(hist_features_30d)\n",
    "for col in hist_df.columns:\n",
    "    test_df[col] = hist_df[col]\n",
    "\n",
    "# PO in window\n",
    "print(\"  Computing POs in test windows...\")\n",
    "po_in_window_test = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    mask = (\n",
    "        (daily_pos['rm_id'] == row['rm_id']) &\n",
    "        (daily_pos['date'] >= row['forecast_start_date']) &\n",
    "        (daily_pos['date'] <= row['forecast_end_date'])\n",
    "    )\n",
    "    po_in_window_test.append(daily_pos.loc[mask, 'po_quantity'].sum())\n",
    "\n",
    "test_df['po_in_window'] = po_in_window_test\n",
    "\n",
    "# Year-over-year\n",
    "print(\"  Computing year-over-year...\")\n",
    "yoy_test = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    start_ly = row['forecast_start_date'] - timedelta(days=365)\n",
    "    end_ly = row['forecast_end_date'] - timedelta(days=365)\n",
    "    \n",
    "    mask = (\n",
    "        (daily_receivals['rm_id'] == row['rm_id']) &\n",
    "        (daily_receivals['date'] >= start_ly) &\n",
    "        (daily_receivals['date'] <= end_ly)\n",
    "    )\n",
    "    \n",
    "    yoy_test.append(daily_receivals.loc[mask, 'weight_sum'].sum())\n",
    "\n",
    "test_df['same_period_last_year'] = yoy_test\n",
    "\n",
    "# ‚Üê ADD ADVANCED FEATURES (SAME AS TRAINING)\n",
    "print(\"  Computing advanced features...\")\n",
    "\n",
    "advanced_features_test = []\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    rm_id = row['rm_id']\n",
    "    ref_date = row['forecast_start_date']\n",
    "    \n",
    "    # VOLATILITY\n",
    "    rec_hist = daily_receivals[\n",
    "        (daily_receivals['rm_id'] == rm_id) &\n",
    "        (daily_receivals['date'] >= ref_date - timedelta(days=30)) &\n",
    "        (daily_receivals['date'] < ref_date)\n",
    "    ]\n",
    "    volatility = rec_hist['weight_sum'].std() if len(rec_hist) > 1 else 0\n",
    "    \n",
    "    # MOVING AVERAGES\n",
    "    ma_7 = rec_hist['weight_sum'].tail(7).mean() if len(rec_hist) >= 7 else 0\n",
    "    ma_30 = rec_hist['weight_sum'].mean()\n",
    "    \n",
    "    # LAG FEATURES\n",
    "    lag_60_start = ref_date - timedelta(days=120)\n",
    "    lag_60_end = ref_date - timedelta(days=60)\n",
    "    lag_60_data = daily_receivals[\n",
    "        (daily_receivals['rm_id'] == rm_id) &\n",
    "        (daily_receivals['date'] >= lag_60_start) &\n",
    "        (daily_receivals['date'] <= lag_60_end)\n",
    "    ]\n",
    "    lag_60 = lag_60_data['weight_sum'].sum()\n",
    "    \n",
    "    advanced_features_test.append({\n",
    "        'volatility_30d': volatility,\n",
    "        'ma_7d': ma_7,\n",
    "        'ma_30d': ma_30,\n",
    "        'lag_60d': lag_60\n",
    "    })\n",
    "\n",
    "adv_df_test = pd.DataFrame(advanced_features_test)\n",
    "for col in adv_df_test.columns:\n",
    "    test_df[col] = adv_df_test[col]\n",
    "\n",
    "# ‚Üê USE SAME feature_cols AS TRAINING (15 features)\n",
    "feature_cols_test = [\n",
    "    'rm_id', 'window_days', 'end_month', 'end_quarter', \n",
    "    'end_dayofyear', 'is_q1',\n",
    "    'rec_hist_30d_sum', 'rec_hist_30d_mean',\n",
    "    'po_hist_30d_sum', 'po_in_window', \n",
    "    'same_period_last_year',\n",
    "    'volatility_30d', 'ma_7d', 'ma_30d', 'lag_60d'  # ‚Üê 15 TOTAL\n",
    "]\n",
    "\n",
    "X_test = test_df[feature_cols_test].copy()\n",
    "X_test['rm_id'] = X_test['rm_id'].astype(float)\n",
    "\n",
    "print(f\"‚úÖ Test features ready: {X_test.shape}\")\n",
    "print(f\"   Features: {len(feature_cols_test)} (same as training)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1826a72",
   "metadata": {},
   "source": [
    "## 14. Generate Predictions (WITH INVERSE LOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87ed8413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Generating predictions...\n",
      "‚úÖ Generated 30450 predictions\n",
      "   Prediction stats (original scale):\n",
      "   - min=1317.61 kg\n",
      "   - max=1814707.80 kg\n",
      "   - mean=59857.73 kg\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 14. Generate Predictions\n",
    "\n",
    "# %%\n",
    "print(\"üéØ Generating predictions...\")\n",
    "\n",
    "# Predict (ORIGINAL SCALE - NO log)\n",
    "predictions = final_model.predict(X_test)\n",
    "predictions = np.maximum(predictions, 0)  # No negative\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'predicted_weight': predictions,\n",
    "    'rm_id': test_df['rm_id'],\n",
    "    'forecast_end_date': test_df['forecast_end_date']\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Generated {len(submission)} predictions\")\n",
    "print(f\"   Prediction stats (original scale):\")\n",
    "print(f\"   - min={predictions.min():.2f} kg\")\n",
    "print(f\"   - max={predictions.max():.2f} kg\")\n",
    "print(f\"   - mean={predictions.mean():.2f} kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4e890b",
   "metadata": {},
   "source": [
    "## 15. Enforce Monotonicity (Cumulative Property)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5799cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Enforcing cumulative monotonicity (CORRECTED)...\n",
      "Before monotonicity: min=1317.61, max=1814707.80\n",
      "After monotonicity: min=1709.15, max=1814707.80\n",
      "‚úÖ Monotonicity enforced\n",
      "\n",
      "üìã Verification - Sample material progressions:\n",
      "\n",
      "  Material 365: ‚úÖ MONOTONIC\n",
      "    First 10 predictions: [4476.2145814  4476.2145814  4476.2145814  4476.2145814  4476.2145814\n",
      " 4476.2145814  4476.2145814  4656.47810405 4656.47810405 4656.47810405]\n",
      "    Min: 4476, Max: 11574\n",
      "\n",
      "  Material 379: ‚úÖ MONOTONIC\n",
      "    First 10 predictions: [3773.71263 3773.71263 3773.71263 3773.71263 3773.71263 3773.71263\n",
      " 3773.71263 3773.71263 3773.71263 3773.71263]\n",
      "    Min: 3774, Max: 3793\n",
      "\n",
      "  Material 389: ‚úÖ MONOTONIC\n",
      "    First 10 predictions: [4194.19489016 4194.19489016 4194.19489016 4194.19489016 4194.19489016\n",
      " 4194.19489016 4194.19489016 4194.19489016 4194.19489016 4194.19489016]\n",
      "    Min: 4194, Max: 13214\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß Enforcing cumulative monotonicity (CORRECTED)...\")\n",
    "\n",
    "# Create working copy with metadata from test_df DIRECTLY (NOT submission)\n",
    "submission_work = pd.DataFrame({\n",
    "    'ID': test_df['ID'].values,\n",
    "    'rm_id': test_df['rm_id'].values,\n",
    "    'forecast_start_date': test_df['forecast_start_date'].values,\n",
    "    'forecast_end_date': test_df['forecast_end_date'].values,\n",
    "    'predicted_weight': predictions\n",
    "})\n",
    "\n",
    "print(f\"Before monotonicity: min={submission_work['predicted_weight'].min():.2f}, max={submission_work['predicted_weight'].max():.2f}\")\n",
    "\n",
    "# Sort by rm_id, then by forecast_end_date for proper cumulative ordering\n",
    "submission_work = submission_work.sort_values(['rm_id', 'forecast_end_date']).reset_index(drop=True)\n",
    "\n",
    "# Apply cummax DENTRO OGNI MATERIALE\n",
    "submission_work['predicted_weight'] = submission_work.groupby('rm_id')['predicted_weight'].cummax()\n",
    "\n",
    "print(f\"After monotonicity: min={submission_work['predicted_weight'].min():.2f}, max={submission_work['predicted_weight'].max():.2f}\")\n",
    "\n",
    "# Sort back by ID for submission format\n",
    "submission_work = submission_work.sort_values('ID').reset_index(drop=True)\n",
    "\n",
    "# Final submission\n",
    "final_submission = submission_work[['ID', 'predicted_weight']].copy()\n",
    "\n",
    "print(f\"‚úÖ Monotonicity enforced\")\n",
    "\n",
    "# Detailed verification\n",
    "print(f\"\\nüìã Verification - Sample material progressions:\")\n",
    "for rm_id in submission_work['rm_id'].unique()[:3]:\n",
    "    mat_subset = submission_work[submission_work['rm_id'] == rm_id].sort_values('ID').head(30)\n",
    "    is_monotonic = (mat_subset['predicted_weight'].diff().dropna() >= -0.01).all()  # Allow tiny floating point errors\n",
    "    status = \"‚úÖ MONOTONIC\" if is_monotonic else \"‚ùå NOT MONOTONIC\"\n",
    "    \n",
    "    print(f\"\\n  Material {rm_id}: {status}\")\n",
    "    print(f\"    First 10 predictions: {mat_subset['predicted_weight'].head(10).values}\")\n",
    "    print(f\"    Min: {mat_subset['predicted_weight'].min():.0f}, Max: {mat_subset['predicted_weight'].max():.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f308b5",
   "metadata": {},
   "source": [
    "## 16. Save Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44af00ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ Submission saved to: gradient_boosting_submission.csv\n",
      "\n",
      "üìä Final Submission Statistics:\n",
      "count    3.045000e+04\n",
      "mean     6.043853e+04\n",
      "std      2.248198e+05\n",
      "min      1.709150e+03\n",
      "25%      3.792847e+03\n",
      "50%      8.870413e+03\n",
      "75%      1.387443e+04\n",
      "max      1.814708e+06\n",
      "Name: predicted_weight, dtype: float64\n",
      "\n",
      "‚úÖ Sanity Checks:\n",
      "  - Total predictions: 30450\n",
      "  - Negative predictions: 0\n",
      "  - Zero predictions: 0\n",
      "  - Missing values: 0\n",
      "\n",
      "üîç CRITICAL VERIFICATION:\n",
      "  Monotonicity violations: 0\n",
      "  ‚úÖ ALL GOOD - Monotonicity is enforced!\n"
     ]
    }
   ],
   "source": [
    "output_file = 'gradient_boosting_submission.csv'\n",
    "final_submission.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nüéâ Submission saved to: {output_file}\")\n",
    "print(f\"\\nüìä Final Submission Statistics:\")\n",
    "print(final_submission['predicted_weight'].describe())\n",
    "\n",
    "# Sanity checks\n",
    "print(f\"\\n‚úÖ Sanity Checks:\")\n",
    "print(f\"  - Total predictions: {len(final_submission)}\")\n",
    "print(f\"  - Negative predictions: {(final_submission['predicted_weight'] < 0).sum()}\")\n",
    "print(f\"  - Zero predictions: {(final_submission['predicted_weight'] == 0).sum()}\")\n",
    "print(f\"  - Missing values: {final_submission['predicted_weight'].isna().sum()}\")\n",
    "\n",
    "# CRITICAL: Verify monotonicity in final submission\n",
    "print(f\"\\nüîç CRITICAL VERIFICATION:\")\n",
    "non_monotonic_count = 0\n",
    "for rm_id in submission_work['rm_id'].unique():\n",
    "    mat_data = final_submission.merge(\n",
    "        submission_work[['ID', 'rm_id', 'forecast_end_date']], \n",
    "        on='ID'\n",
    "    ).sort_values('forecast_end_date')\n",
    "    mat_data = mat_data[mat_data['rm_id'] == rm_id]\n",
    "    \n",
    "    violations = (mat_data['predicted_weight'].diff().dropna() < -0.01).sum()\n",
    "    non_monotonic_count += violations\n",
    "\n",
    "print(f\"  Monotonicity violations: {non_monotonic_count}\")\n",
    "if non_monotonic_count == 0:\n",
    "    print(f\"  ‚úÖ ALL GOOD - Monotonicity is enforced!\")\n",
    "else:\n",
    "    print(f\"  ‚ùå PROBLEM - Monotonicity failed for {non_monotonic_count} transitions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0ad2b0",
   "metadata": {},
   "source": [
    "## 17. Historical Validation: Compare with 2024 Actual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b50a5c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Historical Validation: Comparing model predictions with 2024 actuals...\n",
      "  Computing advanced features...\n",
      "‚úÖ Advanced features added\n",
      "\n",
      "üìä Historical Validation Results (2024 Data):\n",
      "============================================================\n",
      "Sample size: 60 windows from top 10 materials\n",
      "\n",
      "Metric                                   Value\n",
      "------------------------------------------------------------\n",
      "Mean Actual Weight                     294,663 kg\n",
      "Mean Predicted Weight                  215,241 kg\n",
      "Median Actual Weight                   103,286 kg\n",
      "Median Predicted Weight                 74,228 kg\n",
      "\n",
      "Mean Absolute Error (MAE)               98,402 kg\n",
      "Median Absolute Error                   23,212 kg\n",
      "Mean Percentage Error                 354126.8%\n",
      "\n",
      "üìà Performance by Window Length:\n",
      "============================================================\n",
      "    7 days: MAE =       10,391 kg (avg actual:            0 kg)\n",
      "   30 days: MAE =       38,259 kg (avg actual:       80,382 kg)\n",
      "   60 days: MAE =       34,043 kg (avg actual:      186,854 kg)\n",
      "   90 days: MAE =       16,573 kg (avg actual:      304,732 kg)\n",
      "  120 days: MAE =      178,329 kg (avg actual:      519,073 kg)\n",
      "  151 days: MAE =      312,814 kg (avg actual:      676,934 kg)\n",
      "\n",
      "üîç Sample Predictions (First 10 rows):\n",
      " rm_id  window_days  actual_weight  predicted_weight         error\n",
      "2130.0            7            0.0      3.923807e+04  3.923807e+04\n",
      "2130.0           30       399452.0      5.743894e+05  1.749374e+05\n",
      "2130.0           60      1042661.0      1.150644e+06  1.079828e+05\n",
      "2130.0           90      1526669.0      1.507910e+06 -1.875902e+04\n",
      "2130.0          120      2798918.0      1.524868e+06 -1.274050e+06\n",
      "2130.0          151      3549704.0      1.524868e+06 -2.024836e+06\n",
      "2160.0            7            0.0      3.976516e+03  3.976516e+03\n",
      "2160.0           30            0.0      1.127496e+04  1.127496e+04\n",
      "2160.0           60            0.0      1.805826e+04  1.805826e+04\n",
      "2160.0           90            0.0      3.300466e+03  3.300466e+03\n",
      "\n",
      "‚úÖ Validation complete! Check if predictions align with 2024 actuals.\n"
     ]
    }
   ],
   "source": [
    "# Simulate the same prediction windows on 2024 data to validate model performance\n",
    "\n",
    "\n",
    "print(\"üîç Historical Validation: Comparing model predictions with 2024 actuals...\")\n",
    "\n",
    "\n",
    "# Create \"submission-like\" windows for 2024 (Jan 1 - May 31)\n",
    "validation_windows = []\n",
    "\n",
    "\n",
    "# Select top 10 materials for detailed analysis\n",
    "top_10_materials = daily_receivals.groupby('rm_id').size().sort_values(ascending=False).head(10).index.tolist()\n",
    "\n",
    "\n",
    "for rm_id in top_10_materials:\n",
    "    for window_days in [7, 30, 60, 90, 120, 151]:\n",
    "        start_date = datetime(2024, 1, 1)\n",
    "        end_date = start_date + timedelta(days=window_days - 1)\n",
    "        \n",
    "        # Calculate ACTUAL cumulative weight from historical data\n",
    "        actual_weight = calculate_window_receivals(\n",
    "            daily_receivals, start_date, end_date, rm_id\n",
    "        )\n",
    "        \n",
    "        validation_windows.append({\n",
    "            'rm_id': rm_id,\n",
    "            'window_days': window_days,\n",
    "            'end_date': end_date,\n",
    "            'actual_weight': actual_weight\n",
    "        })\n",
    "\n",
    "\n",
    "validation_df = pd.DataFrame(validation_windows)\n",
    "\n",
    "\n",
    "# Create features for these validation windows (same as test)\n",
    "validation_df['start_date'] = datetime(2024, 1, 1)\n",
    "validation_df['end_month'] = validation_df['end_date'].dt.month\n",
    "validation_df['end_quarter'] = validation_df['end_date'].dt.quarter\n",
    "validation_df['end_dayofyear'] = validation_df['end_date'].dt.dayofyear\n",
    "validation_df['is_q1'] = (validation_df['end_quarter'] == 1).astype(int)\n",
    "\n",
    "\n",
    "# Historical features (30d before 2024-01-01)\n",
    "ref_date = datetime(2024, 1, 1)\n",
    "hist_features_val = []\n",
    "\n",
    "\n",
    "for _, row in validation_df.iterrows():\n",
    "    rm_id = row['rm_id']\n",
    "    \n",
    "    rec_stats = calculate_historical_stats(daily_receivals, ref_date, rm_id, 30)\n",
    "    \n",
    "    po_mask = (\n",
    "        (daily_pos['rm_id'] == rm_id) &\n",
    "        (daily_pos['date'] >= ref_date - timedelta(days=30)) &\n",
    "        (daily_pos['date'] < ref_date)\n",
    "    )\n",
    "    po_sum = daily_pos.loc[po_mask, 'po_quantity'].sum()\n",
    "    \n",
    "    # PO in window\n",
    "    po_window_mask = (\n",
    "        (daily_pos['rm_id'] == rm_id) &\n",
    "        (daily_pos['date'] >= row['start_date']) &\n",
    "        (daily_pos['date'] <= row['end_date'])\n",
    "    )\n",
    "    po_in_win = daily_pos.loc[po_window_mask, 'po_quantity'].sum()\n",
    "    \n",
    "    # YoY\n",
    "    start_ly = row['start_date'] - timedelta(days=365)\n",
    "    end_ly = row['end_date'] - timedelta(days=365)\n",
    "    rec_ly_mask = (\n",
    "        (daily_receivals['rm_id'] == rm_id) &\n",
    "        (daily_receivals['date'] >= start_ly) &\n",
    "        (daily_receivals['date'] <= end_ly)\n",
    "    )\n",
    "    yoy = daily_receivals.loc[rec_ly_mask, 'weight_sum'].sum()\n",
    "    \n",
    "    hist_features_val.append({\n",
    "        'rec_hist_30d_sum': rec_stats['sum'],\n",
    "        'rec_hist_30d_mean': rec_stats['mean'],\n",
    "        'po_hist_30d_sum': po_sum,\n",
    "        'po_in_window': po_in_win,\n",
    "        'same_period_last_year': yoy\n",
    "    })\n",
    "\n",
    "\n",
    "hist_val_df = pd.DataFrame(hist_features_val)\n",
    "for col in hist_val_df.columns:\n",
    "    validation_df[col] = hist_val_df[col]\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# ‚Üê FIX: ADD ADVANCED FEATURES (SAME AS TEST SECTION - Section 13)\n",
    "# =========================================================================\n",
    "\n",
    "print(\"  Computing advanced features...\")\n",
    "\n",
    "advanced_features_validation = []\n",
    "\n",
    "for idx, row in validation_df.iterrows():\n",
    "    rm_id = row['rm_id']\n",
    "    ref_date = row['start_date']\n",
    "    \n",
    "    # VOLATILITY (std dev dei 30 giorni precedenti)\n",
    "    rec_hist = daily_receivals[\n",
    "        (daily_receivals['rm_id'] == rm_id) &\n",
    "        (daily_receivals['date'] >= ref_date - timedelta(days=30)) &\n",
    "        (daily_receivals['date'] < ref_date)\n",
    "    ]\n",
    "    volatility = rec_hist['weight_sum'].std() if len(rec_hist) > 1 else 0\n",
    "    \n",
    "    # MOVING AVERAGES\n",
    "    ma_7 = rec_hist['weight_sum'].tail(7).mean() if len(rec_hist) >= 7 else 0\n",
    "    ma_30 = rec_hist['weight_sum'].mean()\n",
    "    \n",
    "    # LAG FEATURES (deliveries 60 giorni fa)\n",
    "    lag_60_start = ref_date - timedelta(days=120)\n",
    "    lag_60_end = ref_date - timedelta(days=60)\n",
    "    lag_60_data = daily_receivals[\n",
    "        (daily_receivals['rm_id'] == rm_id) &\n",
    "        (daily_receivals['date'] >= lag_60_start) &\n",
    "        (daily_receivals['date'] <= lag_60_end)\n",
    "    ]\n",
    "    lag_60 = lag_60_data['weight_sum'].sum()\n",
    "    \n",
    "    advanced_features_validation.append({\n",
    "        'volatility_30d': volatility,\n",
    "        'ma_7d': ma_7,\n",
    "        'ma_30d': ma_30,\n",
    "        'lag_60d': lag_60\n",
    "    })\n",
    "\n",
    "adv_df_validation = pd.DataFrame(advanced_features_validation)\n",
    "for col in adv_df_validation.columns:\n",
    "    validation_df[col] = adv_df_validation[col]\n",
    "\n",
    "print(f\"‚úÖ Advanced features added\")\n",
    "\n",
    "# =========================================================================\n",
    "\n",
    "\n",
    "# Prepare features (NOW ALL 15 COLUMNS ARE PRESENT!)\n",
    "X_val_check = validation_df[feature_cols_test].copy()\n",
    "X_val_check['rm_id'] = X_val_check['rm_id'].astype(float)\n",
    "\n",
    "\n",
    "# Predict\n",
    "validation_df['predicted_weight'] = final_model.predict(X_val_check)\n",
    "validation_df['predicted_weight'] = validation_df['predicted_weight'].clip(lower=0)\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "validation_df['error'] = validation_df['predicted_weight'] - validation_df['actual_weight']\n",
    "validation_df['abs_error'] = validation_df['error'].abs()\n",
    "validation_df['pct_error'] = (validation_df['error'] / validation_df['actual_weight'].replace(0, 1)) * 100\n",
    "\n",
    "\n",
    "print(f\"\\nüìä Historical Validation Results (2024 Data):\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Sample size: {len(validation_df)} windows from top 10 materials\")\n",
    "print(f\"\\n{'Metric':<30} {'Value':>15}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"{'Mean Actual Weight':<30} {validation_df['actual_weight'].mean():>15,.0f} kg\")\n",
    "print(f\"{'Mean Predicted Weight':<30} {validation_df['predicted_weight'].mean():>15,.0f} kg\")\n",
    "print(f\"{'Median Actual Weight':<30} {validation_df['actual_weight'].median():>15,.0f} kg\")\n",
    "print(f\"{'Median Predicted Weight':<30} {validation_df['predicted_weight'].median():>15,.0f} kg\")\n",
    "print(f\"\\n{'Mean Absolute Error (MAE)':<30} {validation_df['abs_error'].mean():>15,.0f} kg\")\n",
    "print(f\"{'Median Absolute Error':<30} {validation_df['abs_error'].median():>15,.0f} kg\")\n",
    "print(f\"{'Mean Percentage Error':<30} {validation_df['pct_error'].mean():>15.1f}%\")\n",
    "\n",
    "\n",
    "# Detailed breakdown by window length\n",
    "print(f\"\\nüìà Performance by Window Length:\")\n",
    "print(f\"{'='*60}\")\n",
    "for window in [7, 30, 60, 90, 120, 151]:\n",
    "    subset = validation_df[validation_df['window_days'] == window]\n",
    "    if len(subset) > 0:\n",
    "        mae = subset['abs_error'].mean()\n",
    "        print(f\"  {window:3d} days: MAE = {mae:>12,.0f} kg (avg actual: {subset['actual_weight'].mean():>12,.0f} kg)\")\n",
    "\n",
    "\n",
    "# Show examples\n",
    "print(f\"\\nüîç Sample Predictions (First 10 rows):\")\n",
    "print(validation_df[['rm_id', 'window_days', 'actual_weight', 'predicted_weight', 'error']].head(10).to_string(index=False))\n",
    "\n",
    "\n",
    "print(f\"\\n‚úÖ Validation complete! Check if predictions align with 2024 actuals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b83183",
   "metadata": {},
   "source": [
    "## 17.5 Refined Validation: Only Active Materials in 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc55e479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Refined Validation: Only materials ACTIVE in 2024...\n",
      "\n",
      "Active windows in 2024: 39 out of 60\n",
      "Inactive materials (0 actual weight): 21\n",
      "\n",
      "üìä Refined Results (Only Active Materials):\n",
      "=================================================================\n",
      "Metric                                                  Value\n",
      "-----------------------------------------------------------------\n",
      "Mean Actual Weight                                    453,327 kg\n",
      "Mean Predicted Weight                                 325,692 kg\n",
      "Median Actual Weight                                  203,255 kg\n",
      "Median Predicted Weight                               178,871 kg\n",
      "\n",
      "Mean Absolute Error (MAE)                             145,939 kg\n",
      "Median Absolute Error                                  30,839 kg\n",
      "Mean Absolute % Error (MAPE)                             25.3%\n",
      "Median Absolute % Error                                  22.8%\n",
      "\n",
      "üìà Performance by Window Length (Active Only):\n",
      "=================================================================\n",
      "    7 days: No active data in 2024\n",
      "   30 days:  7 windows | MAE=    47,458 kg | MAPE=  38.3%\n",
      "   60 days:  8 windows | MAE=    38,044 kg | MAPE=  26.8%\n",
      "   90 days:  8 windows | MAE=    19,897 kg | MAPE=   8.3%\n",
      "  120 days:  8 windows | MAE=   221,966 kg | MAPE=  22.0%\n",
      "  151 days:  8 windows | MAE=   390,019 kg | MAPE=  32.7%\n",
      "\n",
      "üéØ Best Predictions (Lowest Error):\n",
      " rm_id  window_days  actual_weight  predicted_weight  pct_error\n",
      "2131.0           90       157773.0     156252.245242  -0.963888\n",
      "2140.0           90       249660.0     253015.865063   1.344174\n",
      "2134.0           90       332757.0     338443.837441   1.709006\n",
      "2144.0           90       145454.0     152061.577999   4.542727\n",
      "2144.0           30        30740.0      24016.687514 -21.871544\n",
      "\n",
      "‚ö†Ô∏è Worst Predictions (Highest Error):\n",
      " rm_id  window_days  actual_weight  predicted_weight  pct_error\n",
      "2130.0          151      3549704.0      1.524868e+06 -57.042394\n",
      "2130.0          120      2798918.0      1.524868e+06 -45.519381\n",
      "2140.0          151      1046440.0      6.279665e+05 -39.990201\n",
      "2135.0          151       494030.0      2.563284e+05 -48.114820\n",
      "2134.0          151       612846.0      3.953898e+05 -35.483016\n",
      "\n",
      "‚úÖ GOOD - Model predictions are reliable\n",
      "Mean MAPE: 25.3%\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Refined Validation: Only materials ACTIVE in 2024...\\n\")\n",
    "\n",
    "# Filter: keep only rows where actual_weight > 0 (has real 2024 data)\n",
    "validation_active = validation_df[validation_df['actual_weight'] > 0].copy()\n",
    "\n",
    "print(f\"Active windows in 2024: {len(validation_active)} out of {len(validation_df)}\")\n",
    "print(f\"Inactive materials (0 actual weight): {len(validation_df) - len(validation_active)}\")\n",
    "\n",
    "if len(validation_active) > 0:\n",
    "    # Recalculate metrics ONLY for active windows\n",
    "    validation_active['error'] = (validation_active['predicted_weight'] - \n",
    "                                  validation_active['actual_weight'])\n",
    "    validation_active['abs_error'] = validation_active['error'].abs()\n",
    "    validation_active['pct_error'] = (validation_active['error'] / \n",
    "                                      validation_active['actual_weight']) * 100\n",
    "    \n",
    "    print(f\"\\nüìä Refined Results (Only Active Materials):\")\n",
    "    print(f\"{'='*65}\")\n",
    "    print(f\"{'Metric':<35} {'Value':>25}\")\n",
    "    print(f\"{'-'*65}\")\n",
    "    print(f\"{'Mean Actual Weight':<35} {validation_active['actual_weight'].mean():>25,.0f} kg\")\n",
    "    print(f\"{'Mean Predicted Weight':<35} {validation_active['predicted_weight'].mean():>25,.0f} kg\")\n",
    "    print(f\"{'Median Actual Weight':<35} {validation_active['actual_weight'].median():>25,.0f} kg\")\n",
    "    print(f\"{'Median Predicted Weight':<35} {validation_active['predicted_weight'].median():>25,.0f} kg\")\n",
    "    print(f\"\\n{'Mean Absolute Error (MAE)':<35} {validation_active['abs_error'].mean():>25,.0f} kg\")\n",
    "    print(f\"{'Median Absolute Error':<35} {validation_active['abs_error'].median():>25,.0f} kg\")\n",
    "    print(f\"{'Mean Absolute % Error (MAPE)':<35} {validation_active['pct_error'].abs().mean():>25.1f}%\")\n",
    "    print(f\"{'Median Absolute % Error':<35} {validation_active['pct_error'].abs().median():>25.1f}%\")\n",
    "    \n",
    "    # Breakdown by window\n",
    "    print(f\"\\nüìà Performance by Window Length (Active Only):\")\n",
    "    print(f\"{'='*65}\")\n",
    "    for window in [7, 30, 60, 90, 120, 151]:\n",
    "        subset = validation_active[validation_active['window_days'] == window]\n",
    "        if len(subset) > 0:\n",
    "            mae = subset['abs_error'].mean()\n",
    "            mape = subset['pct_error'].abs().mean()\n",
    "            print(f\"  {window:3d} days: {len(subset):2d} windows | MAE={mae:>10,.0f} kg | MAPE={mape:>6.1f}%\")\n",
    "        else:\n",
    "            print(f\"  {window:3d} days: No active data in 2024\")\n",
    "    \n",
    "    # Show best and worst predictions\n",
    "    print(f\"\\nüéØ Best Predictions (Lowest Error):\")\n",
    "    best = validation_active.nsmallest(5, 'abs_error')[['rm_id', 'window_days', 'actual_weight', 'predicted_weight', 'pct_error']]\n",
    "    print(best.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è Worst Predictions (Highest Error):\")\n",
    "    worst = validation_active.nlargest(5, 'abs_error')[['rm_id', 'window_days', 'actual_weight', 'predicted_weight', 'pct_error']]\n",
    "    print(worst.to_string(index=False))\n",
    "    \n",
    "    # Overall assessment\n",
    "    mape_mean = validation_active['pct_error'].abs().mean()\n",
    "    if mape_mean < 20:\n",
    "        assessment = \"‚úÖ EXCELLENT - Model is very accurate\"\n",
    "    elif mape_mean < 50:\n",
    "        assessment = \"‚úÖ GOOD - Model predictions are reliable\"\n",
    "    elif mape_mean < 100:\n",
    "        assessment = \"‚ö†Ô∏è FAIR - Model predictions are acceptable\"\n",
    "    else:\n",
    "        assessment = \"‚ùå POOR - Model predictions need improvement\"\n",
    "    \n",
    "    print(f\"\\n{assessment}\")\n",
    "    print(f\"Mean MAPE: {mape_mean:.1f}%\")\n",
    "else:\n",
    "    print(\"‚ùå No active materials found in 2024 - cannot validate!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntnu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
