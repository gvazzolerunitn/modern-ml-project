{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89334fed",
   "metadata": {},
   "source": [
    "# LSTM Model for Cumulative Material Weight Prediction\n",
    "## Predicting cumulative weight from January 1, 2025 to specified end dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a6cc3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabling GPU\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2715cd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 12:24:29.133033: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-30 12:24:29.178687: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-30 12:24:29.974868: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-30 12:24:29.974868: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c22342",
   "metadata": {},
   "source": [
    "## 1. Load Data and Prediction Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec6a0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading datasets...\n",
      "✅ Receivals: (122590, 10)\n",
      "✅ Prediction mapping: (30450, 4)\n",
      "✅ Sample submission: (30450, 2)\n",
      "✅ Receivals: (122590, 10)\n",
      "✅ Prediction mapping: (30450, 4)\n",
      "✅ Sample submission: (30450, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"📂 Loading datasets...\")\n",
    "receivals = pd.read_csv('data/kernel/receivals.csv')\n",
    "receivals['date_arrival'] = pd.to_datetime(receivals['date_arrival'], utc=True)\n",
    "\n",
    "prediction_mapping = pd.read_csv('data/prediction_mapping.csv')\n",
    "prediction_mapping['forecast_start_date'] = pd.to_datetime(prediction_mapping['forecast_start_date'], utc=True)\n",
    "prediction_mapping['forecast_end_date'] = pd.to_datetime(prediction_mapping['forecast_end_date'], utc=True)\n",
    "\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "print(f\"✅ Receivals: {receivals.shape}\")\n",
    "print(f\"✅ Prediction mapping: {prediction_mapping.shape}\")\n",
    "print(f\"✅ Sample submission: {sample_submission.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf31be0",
   "metadata": {},
   "source": [
    "## 2. Prepare Cumulative Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f887e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creating training dataset with cumulative targets...\n",
      "✅ Daily aggregations created: (41933, 6)\n"
     ]
    }
   ],
   "source": [
    "def calculate_cumulative_weight(receivals_df, rm_id, start_date, end_date):\n",
    "    \"\"\"Calculate cumulative weight for a material between two dates\"\"\"\n",
    "    mask = (\n",
    "        (receivals_df['rm_id'] == rm_id) & \n",
    "        (receivals_df['date_arrival'] >= start_date) &\n",
    "        (receivals_df['date_arrival'] <= end_date)\n",
    "    )\n",
    "    return receivals_df.loc[mask, 'net_weight'].sum()\n",
    "\n",
    "print(\"🔄 Creating training dataset with cumulative targets...\")\n",
    "\n",
    "# Create daily aggregations per material\n",
    "daily_data = receivals.groupby([\n",
    "    receivals['date_arrival'].dt.date, \n",
    "    'rm_id'\n",
    "]).agg({\n",
    "    'net_weight': ['sum', 'count', 'mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "daily_data.columns = ['date', 'rm_id', 'total_weight', 'num_deliveries', 'avg_weight', 'std_weight']\n",
    "daily_data['date'] = pd.to_datetime(daily_data['date'], utc=True)\n",
    "daily_data['std_weight'] = daily_data['std_weight'].fillna(0)\n",
    "daily_data = daily_data.sort_values(['rm_id', 'date'])\n",
    "\n",
    "print(f\"✅ Daily aggregations created: {daily_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca443996",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6508197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Feature engineering...\n",
      "✅ Features engineered: 21 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"🔧 Feature engineering...\")\n",
    "\n",
    "# Temporal features\n",
    "daily_data['year'] = daily_data['date'].dt.year\n",
    "daily_data['month'] = daily_data['date'].dt.month\n",
    "daily_data['dayofweek'] = daily_data['date'].dt.dayofweek\n",
    "daily_data['dayofyear'] = daily_data['date'].dt.dayofyear\n",
    "\n",
    "# Cyclical encoding\n",
    "daily_data['month_sin'] = np.sin(2 * np.pi * daily_data['month'] / 12)\n",
    "daily_data['month_cos'] = np.cos(2 * np.pi * daily_data['month'] / 12)\n",
    "daily_data['dayofweek_sin'] = np.sin(2 * np.pi * daily_data['dayofweek'] / 7)\n",
    "daily_data['dayofweek_cos'] = np.cos(2 * np.pi * daily_data['dayofweek'] / 7)\n",
    "\n",
    "# Lag features\n",
    "for lag in [1, 7, 30]:\n",
    "    daily_data[f'weight_lag_{lag}'] = daily_data.groupby('rm_id')['total_weight'].shift(lag)\n",
    "\n",
    "# Rolling statistics\n",
    "for window in [7, 30]:\n",
    "    daily_data[f'weight_roll_mean_{window}'] = daily_data.groupby('rm_id')['total_weight'].transform(\n",
    "        lambda x: x.rolling(window, min_periods=1).mean()\n",
    "    )\n",
    "    daily_data[f'weight_roll_std_{window}'] = daily_data.groupby('rm_id')['total_weight'].transform(\n",
    "        lambda x: x.rolling(window, min_periods=1).std().fillna(0)\n",
    "    )\n",
    "\n",
    "# Fill NaN\n",
    "daily_data = daily_data.fillna(0)\n",
    "\n",
    "print(f\"✅ Features engineered: {daily_data.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6f0bb",
   "metadata": {},
   "source": [
    "## 4. Create Sequences for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "154641bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creating LSTM sequences...\n",
      "✅ Training sequences: (36366, 30, 18)\n",
      "✅ Target shape: (36366,)\n",
      "✅ Training sequences: (36366, 30, 18)\n",
      "✅ Target shape: (36366,)\n"
     ]
    }
   ],
   "source": [
    "def create_cumulative_sequences(data, sequence_length=30):\n",
    "    \"\"\"Create sequences predicting cumulative future weights\"\"\"\n",
    "    \n",
    "    feature_cols = [col for col in data.columns if col not in ['date', 'rm_id', 'total_weight']]\n",
    "    \n",
    "    X_sequences = []\n",
    "    y_cumulative = []\n",
    "    metadata = []\n",
    "    \n",
    "    for rm_id in data['rm_id'].unique():\n",
    "        material_data = data[data['rm_id'] == rm_id].sort_values('date').copy()\n",
    "        \n",
    "        if len(material_data) < sequence_length + 30:  # Need history + future\n",
    "            continue\n",
    "        \n",
    "        for i in range(sequence_length, len(material_data) - 30):\n",
    "            # Sequence of historical features\n",
    "            seq = material_data[feature_cols].iloc[i-sequence_length:i].values\n",
    "            \n",
    "            # Target: cumulative weight for next 30 days\n",
    "            future_weights = material_data['total_weight'].iloc[i:i+30].sum()\n",
    "            \n",
    "            X_sequences.append(seq)\n",
    "            y_cumulative.append(future_weights)\n",
    "            metadata.append({\n",
    "                'rm_id': rm_id,\n",
    "                'end_date': material_data['date'].iloc[i+29]\n",
    "            })\n",
    "    \n",
    "    return np.array(X_sequences), np.array(y_cumulative), metadata\n",
    "\n",
    "print(\"🔄 Creating LSTM sequences...\")\n",
    "sequence_length = 30\n",
    "\n",
    "X_train, y_train, train_metadata = create_cumulative_sequences(daily_data, sequence_length)\n",
    "\n",
    "print(f\"✅ Training sequences: {X_train.shape}\")\n",
    "print(f\"✅ Target shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba0da6",
   "metadata": {},
   "source": [
    "## 5. Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d73e4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Features scaled\n"
     ]
    }
   ],
   "source": [
    "# Reshape for scaling\n",
    "n_samples, n_timesteps, n_features = X_train.shape\n",
    "X_train_reshaped = X_train.reshape(-1, n_features)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "X_train_scaled = X_train_scaled.reshape(n_samples, n_timesteps, n_features)\n",
    "\n",
    "# Scale target\n",
    "target_scaler = StandardScaler()\n",
    "y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f\"✅ Features scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d3fb4",
   "metadata": {},
   "source": [
    "## 6. Custom Quantile Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4199ee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(alpha=0.2):\n",
    "    \"\"\"Quantile loss for α=0.2\"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        return tf.reduce_mean(tf.maximum(alpha * error, (alpha - 1) * error))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb126edb",
   "metadata": {},
   "source": [
    "## 7. Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb21b256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ Building LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 12:24:38.285752: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-10-30 12:24:38.285786: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:160] env: CUDA_VISIBLE_DEVICES=\"-1\"\n",
      "2025-10-30 12:24:38.285790: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA\n",
      "2025-10-30 12:24:38.285793: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:171] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n",
      "2025-10-30 12:24:38.285796: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:176] retrieving CUDA diagnostic information for host: tuf\n",
      "2025-10-30 12:24:38.285799: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] hostname: tuf\n",
      "2025-10-30 12:24:38.285871: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] libcuda reported version is: 580.95.5\n",
      "2025-10-30 12:24:38.285883: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:194] kernel reported version is: 580.95.5\n",
      "2025-10-30 12:24:38.285885: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:284] kernel version seems to match DSO: 580.95.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">21,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m21,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,593</span> (135.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,593\u001b[0m (135.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,401</span> (134.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,401\u001b[0m (134.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"🏗️ Building LSTM model...\")\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(n_timesteps, n_features)),\n",
    "    Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=quantile_loss(alpha=0.2),\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee3db51",
   "metadata": {},
   "source": [
    "## 8. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fe28513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training model...\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.1182 - mae: 0.4569 - val_loss: 0.0551 - val_mae: 0.2359 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.1182 - mae: 0.4569 - val_loss: 0.0551 - val_mae: 0.2359 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0755 - mae: 0.2999 - val_loss: 0.0555 - val_mae: 0.2283 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0755 - mae: 0.2999 - val_loss: 0.0555 - val_mae: 0.2283 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0665 - mae: 0.2716 - val_loss: 0.0573 - val_mae: 0.2027 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0665 - mae: 0.2716 - val_loss: 0.0573 - val_mae: 0.2027 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0621 - mae: 0.2594 - val_loss: 0.0546 - val_mae: 0.2316 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0621 - mae: 0.2594 - val_loss: 0.0546 - val_mae: 0.2316 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0612 - mae: 0.2589 - val_loss: 0.0619 - val_mae: 0.2308 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0612 - mae: 0.2589 - val_loss: 0.0619 - val_mae: 0.2308 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0597 - mae: 0.2523 - val_loss: 0.0555 - val_mae: 0.2070 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0597 - mae: 0.2523 - val_loss: 0.0555 - val_mae: 0.2070 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0583 - mae: 0.2468 - val_loss: 0.0565 - val_mae: 0.2094 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0583 - mae: 0.2468 - val_loss: 0.0565 - val_mae: 0.2094 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.0574 - mae: 0.2433 - val_loss: 0.0560 - val_mae: 0.2038 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.0574 - mae: 0.2433 - val_loss: 0.0560 - val_mae: 0.2038 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.0570 - mae: 0.2427 - val_loss: 0.0516 - val_mae: 0.1978 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.0570 - mae: 0.2427 - val_loss: 0.0516 - val_mae: 0.1978 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.0564 - mae: 0.2398 - val_loss: 0.0621 - val_mae: 0.2055 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.0564 - mae: 0.2398 - val_loss: 0.0621 - val_mae: 0.2055 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.0560 - mae: 0.2391 - val_loss: 0.0546 - val_mae: 0.2013 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.0560 - mae: 0.2391 - val_loss: 0.0546 - val_mae: 0.2013 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0558 - mae: 0.2377 - val_loss: 0.0579 - val_mae: 0.2181 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0558 - mae: 0.2377 - val_loss: 0.0579 - val_mae: 0.2181 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0557 - mae: 0.2384 - val_loss: 0.0629 - val_mae: 0.1943 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0557 - mae: 0.2384 - val_loss: 0.0629 - val_mae: 0.1943 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m905/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0539 - mae: 0.2300\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0543 - mae: 0.2331 - val_loss: 0.0595 - val_mae: 0.2023 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0543 - mae: 0.2331 - val_loss: 0.0595 - val_mae: 0.2023 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0519 - mae: 0.2232 - val_loss: 0.0672 - val_mae: 0.1882 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0519 - mae: 0.2232 - val_loss: 0.0672 - val_mae: 0.1882 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0508 - mae: 0.2176 - val_loss: 0.0709 - val_mae: 0.2003 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0508 - mae: 0.2176 - val_loss: 0.0709 - val_mae: 0.2003 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0515 - mae: 0.2218 - val_loss: 0.0727 - val_mae: 0.1980 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0515 - mae: 0.2218 - val_loss: 0.0727 - val_mae: 0.1980 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0505 - mae: 0.2173 - val_loss: 0.0650 - val_mae: 0.2025 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0505 - mae: 0.2173 - val_loss: 0.0650 - val_mae: 0.2025 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m905/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0505 - mae: 0.2186\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.0501 - mae: 0.2166 - val_loss: 0.0748 - val_mae: 0.2007 - learning_rate: 5.0000e-04\n",
      "Epoch 19: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "✅ Training completed!\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m910/910\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.0501 - mae: 0.2166 - val_loss: 0.0748 - val_mae: 0.2007 - learning_rate: 5.0000e-04\n",
      "Epoch 19: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "✅ Training completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 Training model...\")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"✅ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413e65e",
   "metadata": {},
   "source": [
    "## 9. Generate Predictions for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09fa2a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Generating submission predictions...\n",
      "Using 18 features for prediction\n",
      "✅ Submission saved: lstm_submission.csv\n",
      "📊 Predictions: 30450\n",
      "📈 Stats:\n",
      "count    3.045000e+04\n",
      "mean     6.416033e+05\n",
      "std      1.541047e+06\n",
      "min      2.966667e+01\n",
      "25%      1.451258e+04\n",
      "50%      6.358117e+04\n",
      "75%      5.218914e+05\n",
      "max      1.952824e+07\n",
      "Name: predicted_weight, dtype: float64\n",
      "✅ Submission saved: lstm_submission.csv\n",
      "📊 Predictions: 30450\n",
      "📈 Stats:\n",
      "count    3.045000e+04\n",
      "mean     6.416033e+05\n",
      "std      1.541047e+06\n",
      "min      2.966667e+01\n",
      "25%      1.451258e+04\n",
      "50%      6.358117e+04\n",
      "75%      5.218914e+05\n",
      "max      1.952824e+07\n",
      "Name: predicted_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"🎯 Generating submission predictions...\")\n",
    "\n",
    "# Define feature columns\n",
    "feature_cols = [col for col in daily_data.columns if col not in ['date', 'rm_id', 'total_weight']]\n",
    "print(f\"Using {len(feature_cols)} features for prediction\")\n",
    "\n",
    "# Group prediction mapping by material\n",
    "predictions_dict = {}\n",
    "\n",
    "for rm_id in prediction_mapping['rm_id'].unique():\n",
    "    material_history = daily_data[daily_data['rm_id'] == rm_id].copy()\n",
    "    \n",
    "    if len(material_history) < sequence_length:\n",
    "        # Fallback: use historical median\n",
    "        predictions_dict[rm_id] = receivals[receivals['rm_id'] == rm_id]['net_weight'].median()\n",
    "        continue\n",
    "    \n",
    "    # Get last sequence\n",
    "    last_sequence = material_history[feature_cols].iloc[-sequence_length:].values\n",
    "    last_sequence_scaled = scaler.transform(last_sequence.reshape(-1, n_features)).reshape(1, sequence_length, n_features)\n",
    "    \n",
    "    # Predict\n",
    "    pred_scaled = model.predict(last_sequence_scaled, verbose=0)\n",
    "    pred_cumulative = target_scaler.inverse_transform(pred_scaled)[0, 0]\n",
    "    \n",
    "    predictions_dict[rm_id] = max(0, pred_cumulative)\n",
    "\n",
    "# Create submission\n",
    "submission = sample_submission.copy()\n",
    "submission['predicted_weight'] = 0\n",
    "\n",
    "for idx, row in prediction_mapping.iterrows():\n",
    "    rm_id = row['rm_id']\n",
    "    end_date = row['forecast_end_date']\n",
    "    start_date = row['forecast_start_date']\n",
    "    \n",
    "    # Calculate days from start to end\n",
    "    days_diff = (end_date - start_date).days\n",
    "    \n",
    "    # Scale prediction by time period\n",
    "    base_prediction = predictions_dict.get(rm_id, receivals['net_weight'].median())\n",
    "    scaled_prediction = base_prediction * (days_diff / 30)  # Scale from 30-day prediction\n",
    "    \n",
    "    submission.loc[submission['ID'] == row['ID'], 'predicted_weight'] = max(0, scaled_prediction)\n",
    "\n",
    "# Ensure cumulative property: predictions should not decrease for same material\n",
    "submission = submission.merge(prediction_mapping[['ID', 'rm_id', 'forecast_end_date']], on='ID')\n",
    "submission = submission.sort_values(['rm_id', 'forecast_end_date'])\n",
    "\n",
    "for rm_id in submission['rm_id'].unique():\n",
    "    mask = submission['rm_id'] == rm_id\n",
    "    submission.loc[mask, 'predicted_weight'] = submission.loc[mask, 'predicted_weight'].cummax()\n",
    "\n",
    "submission = submission[['ID', 'predicted_weight']].sort_values('ID')\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('lstm_submission.csv', index=False)\n",
    "\n",
    "print(f\"✅ Submission saved: lstm_submission.csv\")\n",
    "print(f\"📊 Predictions: {len(submission)}\")\n",
    "print(f\"📈 Stats:\\n{submission['predicted_weight'].describe()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntnu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
